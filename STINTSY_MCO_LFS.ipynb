{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Introduction ##\n",
    "\n",
    "In this notebook, the dataset to be processed is the Labor Force Survey conducted April 2016 and retrieved through Philippine Statistics Authority database. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.stats import ttest_rel\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6.0, 6.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# autoreload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importing LFS PUF April 2016.CSV</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    lfs_data = pd.read_csv(\"src/data/LFS PUF April 2016.CSV\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: CSV file not found. Please make sure the file exists in the correct directory or provide the correct path.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Information, Pre-Processing, and Cleaning</h1>\n",
    "\n",
    "Let's get an overview of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 180862 entries, 0 to 180861\n",
      "Data columns (total 50 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   PUFREG           180862 non-null  int64  \n",
      " 1   PUFPRV           180862 non-null  int64  \n",
      " 2   PUFPRRCD         180862 non-null  int64  \n",
      " 3   PUFHHNUM         180862 non-null  int64  \n",
      " 4   PUFURB2K10       180862 non-null  int64  \n",
      " 5   PUFPWGTFIN       180862 non-null  float64\n",
      " 6   PUFSVYMO         180862 non-null  int64  \n",
      " 7   PUFSVYYR         180862 non-null  int64  \n",
      " 8   PUFPSU           180862 non-null  int64  \n",
      " 9   PUFRPL           180862 non-null  int64  \n",
      " 10  PUFHHSIZE        180862 non-null  int64  \n",
      " 11  PUFC01_LNO       180862 non-null  int64  \n",
      " 12  PUFC03_REL       180862 non-null  int64  \n",
      " 13  PUFC04_SEX       180862 non-null  int64  \n",
      " 14  PUFC05_AGE       180862 non-null  int64  \n",
      " 15  PUFC06_MSTAT     180862 non-null  object \n",
      " 16  PUFC07_GRADE     180862 non-null  object \n",
      " 17  PUFC08_CURSCH    180862 non-null  object \n",
      " 18  PUFC09_GRADTECH  180862 non-null  object \n",
      " 19  PUFC10_CONWR     180862 non-null  object \n",
      " 20  PUFC11_WORK      180862 non-null  object \n",
      " 21  PUFC12_JOB       180862 non-null  object \n",
      " 22  PUFC14_PROCC     180862 non-null  object \n",
      " 23  PUFC16_PKB       180862 non-null  object \n",
      " 24  PUFC17_NATEM     180862 non-null  object \n",
      " 25  PUFC18_PNWHRS    180862 non-null  object \n",
      " 26  PUFC19_PHOURS    180862 non-null  object \n",
      " 27  PUFC20_PWMORE    180862 non-null  object \n",
      " 28  PUFC21_PLADDW    180862 non-null  object \n",
      " 29  PUFC22_PFWRK     180862 non-null  object \n",
      " 30  PUFC23_PCLASS    180862 non-null  object \n",
      " 31  PUFC24_PBASIS    180862 non-null  object \n",
      " 32  PUFC25_PBASIC    180862 non-null  object \n",
      " 33  PUFC26_OJOB      180862 non-null  object \n",
      " 34  PUFC27_NJOBS     180862 non-null  object \n",
      " 35  PUFC28_THOURS    180862 non-null  object \n",
      " 36  PUFC29_WWM48H    180862 non-null  object \n",
      " 37  PUFC30_LOOKW     180862 non-null  object \n",
      " 38  PUFC31_FLWRK     180862 non-null  object \n",
      " 39  PUFC32_JOBSM     180862 non-null  object \n",
      " 40  PUFC33_WEEKS     180862 non-null  object \n",
      " 41  PUFC34_WYNOT     180862 non-null  object \n",
      " 42  PUFC35_LTLOOKW   180862 non-null  object \n",
      " 43  PUFC36_AVAIL     180862 non-null  object \n",
      " 44  PUFC37_WILLING   180862 non-null  object \n",
      " 45  PUFC38_PREVJOB   180862 non-null  object \n",
      " 46  PUFC40_POCC      180862 non-null  object \n",
      " 47  PUFC41_WQTR      180862 non-null  object \n",
      " 48  PUFC43_QKB       180862 non-null  object \n",
      " 49  PUFNEWEMPSTAT    180862 non-null  object \n",
      "dtypes: float64(1), int64(14), object(35)\n",
      "memory usage: 69.0+ MB\n"
     ]
    }
   ],
   "source": [
    "lfs_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Of interest to us, there are:\n",
    "<ul><li>1 contains float values, </li>\n",
    "<li>14 contain integer values, and </li>\n",
    "<li><b>35 are object values</b>.</li></ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's check for duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfs_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicates here, and therefore no cleaning need follow in this regard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset seems to contain null values in the form of whitespaces. Let's count those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Empty Cells:\n",
      "PUFC06_MSTAT        18339\n",
      "PUFC07_GRADE        18339\n",
      "PUFC08_CURSCH      107137\n",
      "PUFC09_GRADTECH     57782\n",
      "PUFC10_CONWR        57782\n",
      "PUFC11_WORK         21894\n",
      "PUFC12_JOB          93306\n",
      "PUFC14_PROCC       108360\n",
      "PUFC16_PKB         108360\n",
      "PUFC17_NATEM       109507\n",
      "PUFC18_PNWHRS      109507\n",
      "PUFC19_PHOURS      109507\n",
      "PUFC20_PWMORE      109507\n",
      "PUFC21_PLADDW      109507\n",
      "PUFC22_PFWRK       109507\n",
      "PUFC23_PCLASS      109507\n",
      "PUFC24_PBASIS      138947\n",
      "PUFC25_PBASIC      144274\n",
      "PUFC26_OJOB        109507\n",
      "PUFC27_NJOBS       174924\n",
      "PUFC28_THOURS      109507\n",
      "PUFC29_WWM48H      163629\n",
      "PUFC30_LOOKW       132692\n",
      "PUFC31_FLWRK       178569\n",
      "PUFC32_JOBSM       178569\n",
      "PUFC33_WEEKS       178569\n",
      "PUFC34_WYNOT       134985\n",
      "PUFC35_LTLOOKW     179269\n",
      "PUFC36_AVAIL       174893\n",
      "PUFC37_WILLING     174893\n",
      "PUFC38_PREVJOB     132692\n",
      "PUFC40_POCC        152982\n",
      "PUFC41_WQTR         81627\n",
      "PUFC43_QKB         107825\n",
      "PUFNEWEMPSTAT       61337\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "has_null = lfs_data.apply(lambda col: col.str.isspace().sum() if col.dtype == 'object' else 0)\n",
    "\n",
    "print(\"Number Empty Cells:\")\n",
    "print(has_null[has_null > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "And standardize, replacing these whitespace values with -1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "lfs_data.replace(r\"^\\s+$\", -1, regex=True, inplace=True)\n",
    "nan_counts_per_column = lfs_data.isna().sum()\n",
    "print(nan_counts_per_column[nan_counts_per_column > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that these are -1, let's return to the data types, and find if our object columns from earlier are convertible to integers (or float):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safely convertable to int:\n",
      "['PUFC06_MSTAT', 'PUFC07_GRADE', 'PUFC08_CURSCH', 'PUFC09_GRADTECH', 'PUFC10_CONWR', 'PUFC11_WORK', 'PUFC12_JOB', 'PUFC14_PROCC', 'PUFC16_PKB', 'PUFC17_NATEM', 'PUFC18_PNWHRS', 'PUFC19_PHOURS', 'PUFC20_PWMORE', 'PUFC21_PLADDW', 'PUFC22_PFWRK', 'PUFC23_PCLASS', 'PUFC24_PBASIS', 'PUFC25_PBASIC', 'PUFC26_OJOB', 'PUFC27_NJOBS', 'PUFC28_THOURS', 'PUFC29_WWM48H', 'PUFC30_LOOKW', 'PUFC31_FLWRK', 'PUFC32_JOBSM', 'PUFC33_WEEKS', 'PUFC34_WYNOT', 'PUFC35_LTLOOKW', 'PUFC36_AVAIL', 'PUFC37_WILLING', 'PUFC38_PREVJOB', 'PUFC40_POCC', 'PUFC41_WQTR', 'PUFC43_QKB', 'PUFNEWEMPSTAT']\n"
     ]
    }
   ],
   "source": [
    "int_convertible_columns = []\n",
    "\n",
    "for col in lfs_data.columns:\n",
    "    if lfs_data[col].dtypes == 'object':  \n",
    "        try:\n",
    "            float_vals = lfs_data[col].dropna().astype(float)\n",
    "            if (float_vals % 1 == 0).all():\n",
    "                int_convertible_columns.append(col)\n",
    "        except ValueError:\n",
    "            pass \n",
    "\n",
    "print(\"Safely convertable to int:\")\n",
    "print(int_convertible_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "And convert to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = [\n",
    "    'PUFC06_MSTAT', 'PUFC08_CURSCH', 'PUFC09_GRADTECH', 'PUFC10_CONWR', 'PUFC11_WORK', \n",
    "    'PUFC12_JOB', 'PUFC14_PROCC', 'PUFC16_PKB', 'PUFC17_NATEM', 'PUFC18_PNWHRS', \n",
    "    'PUFC19_PHOURS', 'PUFC20_PWMORE', 'PUFC21_PLADDW', 'PUFC22_PFWRK', 'PUFC23_PCLASS', \n",
    "    'PUFC24_PBASIS', 'PUFC25_PBASIC', 'PUFC26_OJOB', 'PUFC27_NJOBS', 'PUFC28_THOURS', \n",
    "    'PUFC29_WWM48H', 'PUFC30_LOOKW', 'PUFC31_FLWRK', 'PUFC32_JOBSM', 'PUFC33_WEEKS', \n",
    "    'PUFC34_WYNOT', 'PUFC35_LTLOOKW', 'PUFC36_AVAIL', 'PUFC37_WILLING', 'PUFC38_PREVJOB', \n",
    "    'PUFC40_POCC', 'PUFC41_WQTR', 'PUFC43_QKB', 'PUFNEWEMPSTAT'\n",
    "]\n",
    "\n",
    "for col in columns_to_convert:\n",
    "    lfs_data[col] = lfs_data[col].astype(int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's also apply the unique() function to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PUFREG                17\n",
       "PUFPRV                86\n",
       "PUFPRRCD             116\n",
       "PUFHHNUM           40880\n",
       "PUFURB2K10             2\n",
       "PUFPWGTFIN         35599\n",
       "PUFSVYMO               1\n",
       "PUFSVYYR               1\n",
       "PUFPSU               975\n",
       "PUFRPL                 4\n",
       "PUFHHSIZE             20\n",
       "PUFC01_LNO            23\n",
       "PUFC03_REL            11\n",
       "PUFC04_SEX             2\n",
       "PUFC05_AGE           100\n",
       "PUFC06_MSTAT           7\n",
       "PUFC07_GRADE          68\n",
       "PUFC08_CURSCH          3\n",
       "PUFC09_GRADTECH        3\n",
       "PUFC10_CONWR           6\n",
       "PUFC11_WORK            3\n",
       "PUFC12_JOB             3\n",
       "PUFC14_PROCC          44\n",
       "PUFC16_PKB            88\n",
       "PUFC17_NATEM           4\n",
       "PUFC18_PNWHRS         17\n",
       "PUFC19_PHOURS        103\n",
       "PUFC20_PWMORE          3\n",
       "PUFC21_PLADDW          3\n",
       "PUFC22_PFWRK           3\n",
       "PUFC23_PCLASS          8\n",
       "PUFC24_PBASIS          9\n",
       "PUFC25_PBASIC       1152\n",
       "PUFC26_OJOB            3\n",
       "PUFC27_NJOBS           6\n",
       "PUFC28_THOURS        111\n",
       "PUFC29_WWM48H          6\n",
       "PUFC30_LOOKW           3\n",
       "PUFC31_FLWRK           3\n",
       "PUFC32_JOBSM           7\n",
       "PUFC33_WEEKS          36\n",
       "PUFC34_WYNOT          10\n",
       "PUFC35_LTLOOKW         4\n",
       "PUFC36_AVAIL           3\n",
       "PUFC37_WILLING         3\n",
       "PUFC38_PREVJOB         3\n",
       "PUFC40_POCC           44\n",
       "PUFC41_WQTR            3\n",
       "PUFC43_QKB            89\n",
       "PUFNEWEMPSTAT          4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfs_data.apply(lambda x: x.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Considering our dataset has 18,000 entries, features with particularly low numbers stand out as questions that have clear, defined choices. Reviewing the [questionnaire](https://psada.psa.gov.ph/catalog/67/download/537), we find that certain questions ask the participant to specify beyond prespecified choices.\n",
    "\n",
    "This column possibly contains \"010,\" which is obviously not an integer. We ensure this column is a string, and check for values not specified in the questionnaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['350' '320' '250' -1 '622' '672' '240' '220' '614' '330' '010' '280'\n",
      " '632' '310' '000' '900' '820' '230' '589' '572' '210' '830' '810' '634'\n",
      " '686' '581' '681' '552' '534' '840' '658' '548' '648' '652' '662' '601'\n",
      " '642' '562' '260' '685' '631' '684' '340' '584' '621' '410' '420' '664'\n",
      " '676' '521' '638' '554' '646' '689' '522' '654' '644' '532' '531' '514'\n",
      " '558' '501' '586' '542' '576' '544' '585' '564']\n"
     ]
    }
   ],
   "source": [
    "lfs_data['PUFC07_GRADE'] = lfs_data['PUFC07_GRADE']\n",
    "valid_codes = [\n",
    "    0, 10,  # No Grade, Preschool\n",
    "    210, 220, 230, 240, 250, 260, 280,  # Elementary\n",
    "    310, 320, 330, 340, 350,  # High School\n",
    "    410, 420,  # Post Secondary; If Graduate Specify\n",
    "    810, 820, 830, 840,  # College; If Graduate Specify\n",
    "    900,  # Post Baccalaureate\n",
    "    np.nan\n",
    "]\n",
    "invalid_rows = lfs_data[~(lfs_data['PUFC07_GRADE'].isin(valid_codes))]\n",
    "\n",
    "unique_invalid_values = invalid_rows['PUFC07_GRADE'].unique()\n",
    "print(unique_invalid_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values 5XX 6XX are not detailed in the questionnaire. As it instructs the participant to specify whether they graduated from post secondary or college, we'll create a new data point to encapsulate these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[700]\n"
     ]
    }
   ],
   "source": [
    "lfs_data.loc[~lfs_data['PUFC07_GRADE'].isin(valid_codes), 'PUFC07_GRADE'] = 700\n",
    "print(lfs_data['PUFC07_GRADE'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PUFC11_WORK\n",
       " 2    87556\n",
       " 1    71412\n",
       "-1    21894\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfs_data['PUFC11_WORK'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strong correlations (|corr| > 0.5 and |corr| < 1):\n",
      "PUFPRV — PUFPRRCD: 1.000\n",
      "PUFREG — PUFHHNUM: 0.995\n",
      "PUFC19_PHOURS — PUFC28_THOURS: 0.972\n",
      "PUFC16_PKB — PUFC43_QKB: 0.969\n",
      "PUFC11_WORK — PUFNEWEMPSTAT: 0.964\n",
      "PUFC41_WQTR — PUFNEWEMPSTAT: 0.878\n",
      "PUFC11_WORK — PUFC41_WQTR: 0.852\n",
      "PUFC31_FLWRK — PUFC38_PREVJOB: -0.795\n",
      "PUFC36_AVAIL — PUFC37_WILLING: 0.785\n",
      "PUFC37_WILLING — PUFNEWEMPSTAT: 0.785\n",
      "PUFC18_PNWHRS — PUFC19_PHOURS: 0.785\n",
      "PUFC18_PNWHRS — PUFC28_THOURS: 0.769\n",
      "PUFPWGTFIN — PUFPSU: 0.709\n",
      "PUFC12_JOB — PUFNEWEMPSTAT: 0.704\n",
      "PUFC05_AGE — PUFC06_MSTAT: 0.701\n",
      "PUFC34_WYNOT — PUFNEWEMPSTAT: 0.631\n",
      "PUFC30_LOOKW — PUFNEWEMPSTAT: 0.625\n",
      "PUFC01_LNO — PUFC03_REL: 0.625\n",
      "PUFC05_AGE — PUFC08_CURSCH: 0.590\n",
      "PUFHHSIZE — PUFC01_LNO: 0.571\n",
      "PUFC01_LNO — PUFC05_AGE: -0.567\n",
      "PUFC20_PWMORE — PUFC21_PLADDW: 0.556\n",
      "PUFC08_CURSCH — PUFC11_WORK: -0.514\n",
      "PUFC08_CURSCH — PUFC34_WYNOT: -0.510\n",
      "PUFC08_CURSCH — PUFNEWEMPSTAT: -0.509\n"
     ]
    }
   ],
   "source": [
    "lfs_data_with_nan = lfs_data.copy()\n",
    "lfs_data_with_nan.replace(-1, np.nan, inplace=True)\n",
    "corr_matrix = lfs_data_with_nan.corr()\n",
    "\n",
    "strong_correlations = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i + 1, len(corr_matrix.columns)): \n",
    "        corr_value = corr_matrix.iloc[i, j]\n",
    "        if (0.5 < corr_value < 1) or (-1 < corr_value < -0.5):\n",
    "            strong_correlations.append((\n",
    "                corr_matrix.index[i], \n",
    "                corr_matrix.columns[j], \n",
    "                corr_value\n",
    "            ))\n",
    "\n",
    "strong_correlations.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "print(\"Strong correlations (|corr| > 0.5 and |corr| < 1):\")\n",
    "for var1, var2, corr in strong_correlations:\n",
    "    print(f\"{var1} — {var2}: {corr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use feature_cols as our predictor variables for PUFC11_WORK. Using Regression, we will evaluate how well we can predict whether or not someone has worked in the past week. We begin with preprocessing.\n",
    "\n",
    "### Logistic Regression (LR)\n",
    "Because this is a classification task, we can choose Logistic Regression as a baseline and a first model. In theory, PUFC11_WORK is a binary classification task, something that Logistic Regression should excel at. Since this uses a sigmoid function to map linear probabilities, between 0 and 1, making this suitable for determining whether someone \"worked\" (1) or \"did not work\" (0) within the past week. Finally, LR is particularly interpetable considering the task we're doing, allowing us to check the significance and coefficients associated with each predictor variable if need be.\n",
    "\n",
    "#### Preprocessing: LR\n",
    "We create a copy of our target variable PUFC11_WORK, and create five 80-20 train-test splits for the non-empty PUFC11 data we have. We'll also perform one-hot encoding, given our feature columns are all categorical. We also map the values of PUFC11, normally [1,2], to [0,1]. In addition, because we've determined that our feature_cols are intentionally unanswered as appropriate, we will be treating these values as empty rather than imputing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for k-fold cross-validation...\n",
      "Training on 127174 samples with 9 features\n",
      "Testing on 31794 samples\n",
      "Features: ['PUFC05_AGE', 'PUFC06_MSTAT', 'PUFC04_SEX', 'PUFC07_GRADE', 'PUFC08_CURSCH', 'PUFC38_PREVJOB', 'PUFC31_FLWRK', 'PUFC30_LOOKW', 'PUFC34_WYNOT']\n",
      "Training on 127174 samples with 9 features\n",
      "Testing on 31794 samples\n",
      "Features: ['PUFC05_AGE', 'PUFC06_MSTAT', 'PUFC04_SEX', 'PUFC07_GRADE', 'PUFC08_CURSCH', 'PUFC38_PREVJOB', 'PUFC31_FLWRK', 'PUFC30_LOOKW', 'PUFC34_WYNOT']\n",
      "Training on 127174 samples with 9 features\n",
      "Testing on 31794 samples\n",
      "Features: ['PUFC05_AGE', 'PUFC06_MSTAT', 'PUFC04_SEX', 'PUFC07_GRADE', 'PUFC08_CURSCH', 'PUFC38_PREVJOB', 'PUFC31_FLWRK', 'PUFC30_LOOKW', 'PUFC34_WYNOT']\n",
      "Training on 127175 samples with 9 features\n",
      "Testing on 31793 samples\n",
      "Features: ['PUFC05_AGE', 'PUFC06_MSTAT', 'PUFC04_SEX', 'PUFC07_GRADE', 'PUFC08_CURSCH', 'PUFC38_PREVJOB', 'PUFC31_FLWRK', 'PUFC30_LOOKW', 'PUFC34_WYNOT']\n",
      "Training on 127175 samples with 9 features\n",
      "Testing on 31793 samples\n",
      "Features: ['PUFC05_AGE', 'PUFC06_MSTAT', 'PUFC04_SEX', 'PUFC07_GRADE', 'PUFC08_CURSCH', 'PUFC38_PREVJOB', 'PUFC31_FLWRK', 'PUFC30_LOOKW', 'PUFC34_WYNOT']\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import prepare_data_kfold\n",
    "\n",
    "target_col = 'PUFC11_WORK'\n",
    "feature_cols = [\n",
    "    'PUFC05_AGE', 'PUFC06_MSTAT', 'PUFC04_SEX', \n",
    "    'PUFC07_GRADE', 'PUFC08_CURSCH', \n",
    "    'PUFC38_PREVJOB', 'PUFC31_FLWRK',\n",
    "    'PUFC30_LOOKW', 'PUFC34_WYNOT'\n",
    "]\n",
    "\n",
    "categorical_cols = feature_cols\n",
    "n_splits = 5\n",
    "missing_value =-1\n",
    "seed = 45\n",
    "\n",
    "folds_data = prepare_data_kfold(lfs_data, target_col = target_col,\n",
    "                         n_splits = n_splits,\n",
    "                         missing_value = missing_value,\n",
    "                         categorical_cols = categorical_cols,\n",
    "                         feature_cols = feature_cols,\n",
    "                         seed = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training: LR\n",
    "We are ready to start training. As we are setting a baseline, we'll start off with these hyperparameters. We'll use Stochastic Gradient Descent as our specified optimizer, though technically implementing mini-batch with a batch size of 128. We are looking for improvements in loss greater than 0.0001, else we stop early within three epochs. Lastly, we use a weight_decay of 0, indicating no regularization for now. All hyperparameters indicated below also indicate their default value if unspecified. For each convergence, we will track the metrics of that fold and aggregate it across all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== FOLD 1 ====================\n",
      "Epoch 1/50: Train Loss: 0.2033, Test Loss: 0.1086, Train Acc: 0.9550, Test Acc: 0.9817, LR: 0.010000\n",
      "Epoch 2/50: Train Loss: 0.0949, Test Loss: 0.0863, Train Acc: 0.9846, Test Acc: 0.9857, LR: 0.010000\n",
      "Epoch 3/50: Train Loss: 0.0818, Test Loss: 0.0789, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.010000\n",
      "Epoch 4/50: Train Loss: 0.0765, Test Loss: 0.0752, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.010000\n",
      "Epoch 5/50: Train Loss: 0.0736, Test Loss: 0.0731, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.005000\n",
      "Epoch 6/50: Train Loss: 0.0721, Test Loss: 0.0723, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.005000\n",
      "Epoch 7/50: Train Loss: 0.0714, Test Loss: 0.0717, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.005000\n",
      "Epoch 8/50: Train Loss: 0.0708, Test Loss: 0.0712, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.005000\n",
      "Epoch 9/50: Train Loss: 0.0702, Test Loss: 0.0707, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.005000\n",
      "Epoch 10/50: Train Loss: 0.0698, Test Loss: 0.0703, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.002500\n",
      "Epoch 11/50: Train Loss: 0.0694, Test Loss: 0.0702, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.002500\n",
      "Epoch 12/50: Train Loss: 0.0693, Test Loss: 0.0700, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.002500\n",
      "Epoch 13/50: Train Loss: 0.0691, Test Loss: 0.0699, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.002500\n",
      "Epoch 14/50: Train Loss: 0.0689, Test Loss: 0.0697, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.002500\n",
      "Epoch 15/50: Train Loss: 0.0688, Test Loss: 0.0696, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.001250\n",
      "Epoch 16/50: Train Loss: 0.0687, Test Loss: 0.0695, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.001250\n",
      "Epoch 17/50: Train Loss: 0.0686, Test Loss: 0.0695, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.001250\n",
      "Epoch 18/50: Train Loss: 0.0686, Test Loss: 0.0694, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.001250\n",
      "Epoch 19/50: Train Loss: 0.0685, Test Loss: 0.0694, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.001250\n",
      "Epoch 20/50: Train Loss: 0.0684, Test Loss: 0.0693, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.000625\n",
      "Epoch 21/50: Train Loss: 0.0683, Test Loss: 0.0693, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.000625\n",
      "Epoch 22/50: Train Loss: 0.0683, Test Loss: 0.0692, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.000625\n",
      "Epoch 23/50: Train Loss: 0.0683, Test Loss: 0.0692, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.000625\n",
      "Epoch 24/50: Train Loss: 0.0683, Test Loss: 0.0692, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.000625\n",
      "Epoch 25/50: Train Loss: 0.0682, Test Loss: 0.0692, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.000313\n",
      "Early stopping triggered after 25 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[14052   250]\n",
      " [  206 17286]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0  0.98555197 0.98251993 0.98403361     14302\n",
      "         1.0  0.98574361 0.98822319 0.98698184     17492\n",
      "\n",
      "    accuracy                      0.98565767     31794\n",
      "   macro avg  0.98564779 0.98537156 0.98550773     31794\n",
      "weighted avg  0.98565741 0.98565767 0.98565563     31794\n",
      "\n",
      "\n",
      "==================== FOLD 2 ====================\n",
      "Epoch 1/50: Train Loss: 0.2028, Test Loss: 0.1088, Train Acc: 0.9556, Test Acc: 0.9821, LR: 0.010000\n",
      "Epoch 2/50: Train Loss: 0.0946, Test Loss: 0.0868, Train Acc: 0.9846, Test Acc: 0.9860, LR: 0.010000\n",
      "Epoch 3/50: Train Loss: 0.0816, Test Loss: 0.0795, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.010000\n",
      "Epoch 4/50: Train Loss: 0.0763, Test Loss: 0.0758, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.010000\n",
      "Epoch 5/50: Train Loss: 0.0735, Test Loss: 0.0736, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.005000\n",
      "Epoch 6/50: Train Loss: 0.0720, Test Loss: 0.0728, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.005000\n",
      "Epoch 7/50: Train Loss: 0.0712, Test Loss: 0.0722, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.005000\n",
      "Epoch 8/50: Train Loss: 0.0706, Test Loss: 0.0717, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.005000\n",
      "Epoch 9/50: Train Loss: 0.0701, Test Loss: 0.0712, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.005000\n",
      "Epoch 10/50: Train Loss: 0.0697, Test Loss: 0.0708, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.002500\n",
      "Epoch 11/50: Train Loss: 0.0694, Test Loss: 0.0706, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.002500\n",
      "Epoch 12/50: Train Loss: 0.0692, Test Loss: 0.0704, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.002500\n",
      "Epoch 13/50: Train Loss: 0.0690, Test Loss: 0.0703, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.002500\n",
      "Epoch 14/50: Train Loss: 0.0688, Test Loss: 0.0701, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.002500\n",
      "Epoch 15/50: Train Loss: 0.0687, Test Loss: 0.0700, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.001250\n",
      "Epoch 16/50: Train Loss: 0.0686, Test Loss: 0.0699, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.001250\n",
      "Epoch 17/50: Train Loss: 0.0686, Test Loss: 0.0699, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.001250\n",
      "Epoch 18/50: Train Loss: 0.0685, Test Loss: 0.0698, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.001250\n",
      "Epoch 19/50: Train Loss: 0.0684, Test Loss: 0.0697, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.001250\n",
      "Epoch 20/50: Train Loss: 0.0684, Test Loss: 0.0697, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.000625\n",
      "Epoch 21/50: Train Loss: 0.0684, Test Loss: 0.0696, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.000625\n",
      "Epoch 22/50: Train Loss: 0.0683, Test Loss: 0.0696, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.000625\n",
      "Epoch 23/50: Train Loss: 0.0682, Test Loss: 0.0696, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.000625\n",
      "Epoch 24/50: Train Loss: 0.0682, Test Loss: 0.0696, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.000625\n",
      "Epoch 25/50: Train Loss: 0.0682, Test Loss: 0.0695, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.000313\n",
      "Early stopping triggered after 25 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13960   226]\n",
      " [  219 17389]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0  0.98455462 0.98406880 0.98431165     14186\n",
      "         1.0  0.98717003 0.98756247 0.98736621     17608\n",
      "\n",
      "    accuracy                      0.98600365     31794\n",
      "   macro avg  0.98586232 0.98581564 0.98583893     31794\n",
      "weighted avg  0.98600307 0.98600365 0.98600331     31794\n",
      "\n",
      "\n",
      "==================== FOLD 3 ====================\n",
      "Epoch 1/50: Train Loss: 0.2034, Test Loss: 0.1080, Train Acc: 0.9550, Test Acc: 0.9829, LR: 0.010000\n",
      "Epoch 2/50: Train Loss: 0.0952, Test Loss: 0.0853, Train Acc: 0.9844, Test Acc: 0.9866, LR: 0.010000\n",
      "Epoch 3/50: Train Loss: 0.0823, Test Loss: 0.0776, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.010000\n",
      "Epoch 4/50: Train Loss: 0.0769, Test Loss: 0.0738, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.010000\n",
      "Epoch 5/50: Train Loss: 0.0740, Test Loss: 0.0715, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.005000\n",
      "Epoch 6/50: Train Loss: 0.0725, Test Loss: 0.0707, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.005000\n",
      "Epoch 7/50: Train Loss: 0.0718, Test Loss: 0.0701, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.005000\n",
      "Epoch 8/50: Train Loss: 0.0712, Test Loss: 0.0695, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.005000\n",
      "Epoch 9/50: Train Loss: 0.0707, Test Loss: 0.0691, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.005000\n",
      "Epoch 10/50: Train Loss: 0.0702, Test Loss: 0.0686, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.002500\n",
      "Epoch 11/50: Train Loss: 0.0699, Test Loss: 0.0685, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.002500\n",
      "Epoch 12/50: Train Loss: 0.0697, Test Loss: 0.0683, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.002500\n",
      "Epoch 13/50: Train Loss: 0.0696, Test Loss: 0.0681, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.002500\n",
      "Epoch 14/50: Train Loss: 0.0694, Test Loss: 0.0680, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.002500\n",
      "Epoch 15/50: Train Loss: 0.0693, Test Loss: 0.0679, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.001250\n",
      "Epoch 16/50: Train Loss: 0.0691, Test Loss: 0.0678, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.001250\n",
      "Epoch 17/50: Train Loss: 0.0691, Test Loss: 0.0677, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.001250\n",
      "Epoch 18/50: Train Loss: 0.0690, Test Loss: 0.0677, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.001250\n",
      "Epoch 19/50: Train Loss: 0.0691, Test Loss: 0.0676, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.001250\n",
      "Epoch 20/50: Train Loss: 0.0689, Test Loss: 0.0675, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.000625\n",
      "Epoch 21/50: Train Loss: 0.0688, Test Loss: 0.0675, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.000625\n",
      "Epoch 22/50: Train Loss: 0.0688, Test Loss: 0.0675, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.000625\n",
      "Epoch 23/50: Train Loss: 0.0688, Test Loss: 0.0675, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.000625\n",
      "Epoch 24/50: Train Loss: 0.0688, Test Loss: 0.0674, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.000625\n",
      "Epoch 25/50: Train Loss: 0.0687, Test Loss: 0.0674, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.000313\n",
      "Early stopping triggered after 25 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[14145   204]\n",
      " [  222 17223]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0  0.98454792 0.98578298 0.98516506     14349\n",
      "         1.0  0.98829403 0.98727429 0.98778390     17445\n",
      "\n",
      "    accuracy                      0.98660125     31794\n",
      "   macro avg  0.98642097 0.98652864 0.98647448     31794\n",
      "weighted avg  0.98660337 0.98660125 0.98660199     31794\n",
      "\n",
      "\n",
      "==================== FOLD 4 ====================\n",
      "Epoch 1/50: Train Loss: 0.2033, Test Loss: 0.1068, Train Acc: 0.9557, Test Acc: 0.9829, LR: 0.010000\n",
      "Epoch 2/50: Train Loss: 0.0952, Test Loss: 0.0845, Train Acc: 0.9846, Test Acc: 0.9865, LR: 0.010000\n",
      "Epoch 3/50: Train Loss: 0.0824, Test Loss: 0.0769, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.010000\n",
      "Epoch 4/50: Train Loss: 0.0770, Test Loss: 0.0731, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.010000\n",
      "Epoch 5/50: Train Loss: 0.0741, Test Loss: 0.0709, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.005000\n",
      "Epoch 6/50: Train Loss: 0.0727, Test Loss: 0.0701, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.005000\n",
      "Epoch 7/50: Train Loss: 0.0719, Test Loss: 0.0694, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.005000\n",
      "Epoch 8/50: Train Loss: 0.0713, Test Loss: 0.0689, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.005000\n",
      "Epoch 9/50: Train Loss: 0.0709, Test Loss: 0.0684, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.005000\n",
      "Epoch 10/50: Train Loss: 0.0704, Test Loss: 0.0680, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.002500\n",
      "Epoch 11/50: Train Loss: 0.0701, Test Loss: 0.0678, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.002500\n",
      "Epoch 12/50: Train Loss: 0.0699, Test Loss: 0.0676, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.002500\n",
      "Epoch 13/50: Train Loss: 0.0697, Test Loss: 0.0675, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.002500\n",
      "Epoch 14/50: Train Loss: 0.0696, Test Loss: 0.0673, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.002500\n",
      "Epoch 15/50: Train Loss: 0.0694, Test Loss: 0.0672, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.001250\n",
      "Epoch 16/50: Train Loss: 0.0693, Test Loss: 0.0671, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.001250\n",
      "Epoch 17/50: Train Loss: 0.0692, Test Loss: 0.0670, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.001250\n",
      "Epoch 18/50: Train Loss: 0.0692, Test Loss: 0.0670, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.001250\n",
      "Epoch 19/50: Train Loss: 0.0691, Test Loss: 0.0669, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.001250\n",
      "Epoch 20/50: Train Loss: 0.0690, Test Loss: 0.0668, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.000625\n",
      "Epoch 21/50: Train Loss: 0.0690, Test Loss: 0.0668, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.000625\n",
      "Epoch 22/50: Train Loss: 0.0689, Test Loss: 0.0668, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.000625\n",
      "Epoch 23/50: Train Loss: 0.0689, Test Loss: 0.0667, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.000625\n",
      "Epoch 24/50: Train Loss: 0.0689, Test Loss: 0.0667, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.000625\n",
      "Epoch 25/50: Train Loss: 0.0689, Test Loss: 0.0667, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.000313\n",
      "Early stopping triggered after 25 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13985   228]\n",
      " [  200 17380]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0  0.98590060 0.98395835 0.98492852     14213\n",
      "         1.0  0.98705134 0.98862344 0.98783676     17580\n",
      "\n",
      "    accuracy                      0.98653792     31793\n",
      "   macro avg  0.98647597 0.98629089 0.98638264     31793\n",
      "weighted avg  0.98653690 0.98653792 0.98653664     31793\n",
      "\n",
      "\n",
      "==================== FOLD 5 ====================\n",
      "Epoch 1/50: Train Loss: 0.2028, Test Loss: 0.1102, Train Acc: 0.9551, Test Acc: 0.9818, LR: 0.010000\n",
      "Epoch 2/50: Train Loss: 0.0944, Test Loss: 0.0881, Train Acc: 0.9846, Test Acc: 0.9857, LR: 0.010000\n",
      "Epoch 3/50: Train Loss: 0.0814, Test Loss: 0.0806, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.010000\n",
      "Epoch 4/50: Train Loss: 0.0761, Test Loss: 0.0769, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.010000\n",
      "Epoch 5/50: Train Loss: 0.0732, Test Loss: 0.0747, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.005000\n",
      "Epoch 6/50: Train Loss: 0.0717, Test Loss: 0.0739, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.005000\n",
      "Epoch 7/50: Train Loss: 0.0710, Test Loss: 0.0733, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.005000\n",
      "Epoch 8/50: Train Loss: 0.0704, Test Loss: 0.0727, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.005000\n",
      "Epoch 9/50: Train Loss: 0.0699, Test Loss: 0.0723, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.005000\n",
      "Epoch 10/50: Train Loss: 0.0695, Test Loss: 0.0718, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.002500\n",
      "Epoch 11/50: Train Loss: 0.0691, Test Loss: 0.0717, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.002500\n",
      "Epoch 12/50: Train Loss: 0.0690, Test Loss: 0.0715, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.002500\n",
      "Epoch 13/50: Train Loss: 0.0688, Test Loss: 0.0713, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.002500\n",
      "Epoch 14/50: Train Loss: 0.0686, Test Loss: 0.0712, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.002500\n",
      "Epoch 15/50: Train Loss: 0.0684, Test Loss: 0.0711, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.001250\n",
      "Epoch 16/50: Train Loss: 0.0684, Test Loss: 0.0710, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.001250\n",
      "Epoch 17/50: Train Loss: 0.0683, Test Loss: 0.0709, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.001250\n",
      "Epoch 18/50: Train Loss: 0.0682, Test Loss: 0.0709, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.001250\n",
      "Epoch 19/50: Train Loss: 0.0682, Test Loss: 0.0708, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.001250\n",
      "Epoch 20/50: Train Loss: 0.0681, Test Loss: 0.0707, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.000625\n",
      "Epoch 21/50: Train Loss: 0.0680, Test Loss: 0.0707, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.000625\n",
      "Epoch 22/50: Train Loss: 0.0680, Test Loss: 0.0707, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.000625\n",
      "Epoch 23/50: Train Loss: 0.0680, Test Loss: 0.0706, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.000625\n",
      "Epoch 24/50: Train Loss: 0.0679, Test Loss: 0.0706, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.000625\n",
      "Epoch 25/50: Train Loss: 0.0679, Test Loss: 0.0706, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.000313\n",
      "Early stopping triggered after 25 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[14137   225]\n",
      " [  229 17202]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0  0.98405959 0.98433366 0.98419660     14362\n",
      "         1.0  0.98708900 0.98686249 0.98697573     17431\n",
      "\n",
      "    accuracy                      0.98572013     31793\n",
      "   macro avg  0.98557429 0.98559807 0.98558617     31793\n",
      "weighted avg  0.98572051 0.98572013 0.98572030     31793\n",
      "\n",
      "==================== AGGREGATE RESULTS ====================\n",
      "\n",
      "Confusion Matrix:\n",
      "[[70279  1133]\n",
      " [ 1076 86480]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0  0.98492047 0.98413432 0.98452724     71412\n",
      "         1.0  0.98706813 0.98771072 0.98738932     87556\n",
      "\n",
      "    accuracy                      0.98610412    158968\n",
      "   macro avg  0.98599430 0.98592252 0.98595828    158968\n",
      "weighted avg  0.98610335 0.98610412 0.98610361    158968\n",
      "\n",
      "[0.985657671258728, 0.986003648487136, 0.9866012455180223, 0.9865379171515742, 0.985720127071997]\n",
      "\n",
      "Model training complete!\n",
      "  Average Final Train Loss:     0.068390\n",
      "  Average Final Test Loss:      0.068672\n",
      "  Average Final Train Accuracy: 0.986104\n",
      "  Average Final Test Accuracy:  0.986104\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.trainEval import *\n",
    "result_dict = train_model(\n",
    "    folds_data,\n",
    "    optimizer_string=\"sgd\", #Stochastic Gradient Descent\n",
    "    scheduler_step_size=5,\n",
    "    learning_rate=0.01,\n",
    "    scheduler_gamma=0.5,\n",
    "    convergence_threshold=1e-4, \n",
    "    num_epochs=50,\n",
    "    patience=3, # Stop at 3 epochs with no improvement\n",
    "    weight_decay=0,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "lr_accuracies_test = result_dict[\"all_final_test_accuracies\"]\n",
    "print(lr_accuracies_test);\n",
    "\n",
    "\n",
    "print(\"\\nModel training complete!\")\n",
    "aggregate_cm = result_dict[\"aggregate_confusion_matrix\"]\n",
    "\n",
    "sns.heatmap(aggregate_cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=[\"Negative\", \"Positive\"], \n",
    "            yticklabels=[\"Negative\", \"Positive\"]) \n",
    "\n",
    "\n",
    "print(f\"  Average Final Train Loss:     {result_dict['aggregated_final_metrics']['avg_final_train_loss']:.6f}\")\n",
    "print(f\"  Average Final Test Loss:      {result_dict['aggregated_final_metrics']['avg_final_test_loss']:.6f}\")\n",
    "print(f\"  Average Final Train Accuracy: {result_dict['aggregated_final_metrics']['avg_final_train_accuracy']:.6f}\")\n",
    "print(f\"  Average Final Test Accuracy:  {result_dict['aggregated_final_metrics']['avg_final_test_accuracy']:.6f}\")\n",
    "\n",
    "plt.title(\"Aggregate Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error: LR\n",
    "This model performed well for an initial run. All folds ran for 25 epochs. Both accuracies for test and train data are high, though there is usually minimal improvement from the third epoch onwards. The train and test loss both see steady improvement, but we see the test loss  to be slightly higher and plateau earlier at around the sixteenth epoch. \n",
    "\n",
    "We see that the train loss is slightly lower than the test loss in folds 1, 2, and 5, while the reverse is true for folds 3 and 4. This would indicate a model that overfits for the former folds, and underfits for the latter. Averaging these out, its clear that with an aggregated train and test loss of 0.684 and 0.687 respectively, the overall performance of the model doesn't significantly overfit nor underfit. This outcome is surprising, considering that we've used nine predictor variables while also forgoing the use of regularization, as these would have been expected to introduce noise and contribute to overfitting through curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning: LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our base parameters having been set, we can begin hyperparameter tuning. We'll implement *Random Search* as to not have to exhaustively branch through each possible combination of parameters. Below are the hyperparameter's we'll use and choose from. For example, if our algorithm randomly, for instance, chooses the second element of each, then it will test Logistic Regression at a learning rate of 0.1, batch size of 64, and the with the ADAM optimizer, etc. \n",
    "\n",
    "Values evenly spaced between 0.000001 and 0.001 will be explored for the learning rates. This will go on for fifty iterations, afterwhich a best model will be selected based on test accuracy. We'll perform random search rather than grid search, as a compromise to searching through all 259,200 combinations of hyperparameters in our grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing configuration:\n",
      "{'weight_decay': np.float64(0.00046415888336127773), 'scheduler_step_size': 5, 'scheduler_gamma': 0.9, 'patience': 5, 'optimizer': 'sgd', 'num_epochs': 30, 'learning_rate': np.float64(0.06951927961775606), 'batch_size': 256}\n",
      "\n",
      "==================== FOLD 1 ====================\n",
      "Epoch 1/30: Train Loss: 0.1207, Test Loss: 0.0773, Train Acc: 0.9766, Test Acc: 0.9857, LR: 0.069519\n",
      "Epoch 2/30: Train Loss: 0.0729, Test Loss: 0.0712, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.069519\n",
      "Epoch 3/30: Train Loss: 0.0692, Test Loss: 0.0693, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.069519\n",
      "Epoch 4/30: Train Loss: 0.0678, Test Loss: 0.0684, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.069519\n",
      "Epoch 5/30: Train Loss: 0.0670, Test Loss: 0.0679, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.062567\n",
      "Epoch 6/30: Train Loss: 0.0665, Test Loss: 0.0677, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.062567\n",
      "Epoch 7/30: Train Loss: 0.0662, Test Loss: 0.0674, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.062567\n",
      "Epoch 8/30: Train Loss: 0.0660, Test Loss: 0.0673, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.062567\n",
      "Epoch 9/30: Train Loss: 0.0658, Test Loss: 0.0672, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.062567\n",
      "Epoch 10/30: Train Loss: 0.0657, Test Loss: 0.0671, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.056311\n",
      "Epoch 11/30: Train Loss: 0.0656, Test Loss: 0.0670, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.056311\n",
      "Epoch 12/30: Train Loss: 0.0655, Test Loss: 0.0670, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.056311\n",
      "Epoch 13/30: Train Loss: 0.0655, Test Loss: 0.0670, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.056311\n",
      "Epoch 14/30: Train Loss: 0.0654, Test Loss: 0.0669, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.056311\n",
      "Epoch 15/30: Train Loss: 0.0654, Test Loss: 0.0669, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.050680\n",
      "Epoch 16/30: Train Loss: 0.0653, Test Loss: 0.0668, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.050680\n",
      "Epoch 17/30: Train Loss: 0.0653, Test Loss: 0.0668, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.050680\n",
      "Epoch 18/30: Train Loss: 0.0653, Test Loss: 0.0669, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.050680\n",
      "Epoch 19/30: Train Loss: 0.0653, Test Loss: 0.0668, Train Acc: 0.9862, Test Acc: 0.9857, LR: 0.050680\n",
      "Early stopping triggered after 19 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[14052   250]\n",
      " [  206 17286]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0  0.98555197 0.98251993 0.98403361     14302\n",
      "         1.0  0.98574361 0.98822319 0.98698184     17492\n",
      "\n",
      "    accuracy                      0.98565767     31794\n",
      "   macro avg  0.98564779 0.98537156 0.98550773     31794\n",
      "weighted avg  0.98565741 0.98565767 0.98565563     31794\n",
      "\n",
      "\n",
      "==================== FOLD 2 ====================\n",
      "Epoch 1/30: Train Loss: 0.1201, Test Loss: 0.0775, Train Acc: 0.9763, Test Acc: 0.9860, LR: 0.069519\n",
      "Epoch 2/30: Train Loss: 0.0727, Test Loss: 0.0713, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.069519\n",
      "Epoch 3/30: Train Loss: 0.0691, Test Loss: 0.0693, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.069519\n",
      "Epoch 4/30: Train Loss: 0.0677, Test Loss: 0.0683, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.069519\n",
      "Epoch 5/30: Train Loss: 0.0670, Test Loss: 0.0678, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.062567\n",
      "Epoch 6/30: Train Loss: 0.0665, Test Loss: 0.0674, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.062567\n",
      "Epoch 7/30: Train Loss: 0.0662, Test Loss: 0.0672, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.062567\n",
      "Epoch 8/30: Train Loss: 0.0660, Test Loss: 0.0670, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.062567\n",
      "Epoch 9/30: Train Loss: 0.0658, Test Loss: 0.0668, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.062567\n",
      "Epoch 10/30: Train Loss: 0.0657, Test Loss: 0.0667, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.056311\n",
      "Epoch 11/30: Train Loss: 0.0656, Test Loss: 0.0667, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.056311\n",
      "Epoch 12/30: Train Loss: 0.0655, Test Loss: 0.0666, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.056311\n",
      "Epoch 13/30: Train Loss: 0.0655, Test Loss: 0.0666, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.056311\n",
      "Epoch 14/30: Train Loss: 0.0654, Test Loss: 0.0665, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.056311\n",
      "Epoch 15/30: Train Loss: 0.0654, Test Loss: 0.0665, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.050680\n",
      "Epoch 16/30: Train Loss: 0.0654, Test Loss: 0.0664, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.050680\n",
      "Epoch 17/30: Train Loss: 0.0653, Test Loss: 0.0664, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.050680\n",
      "Epoch 18/30: Train Loss: 0.0653, Test Loss: 0.0664, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.050680\n",
      "Epoch 19/30: Train Loss: 0.0653, Test Loss: 0.0664, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.050680\n",
      "Epoch 20/30: Train Loss: 0.0653, Test Loss: 0.0663, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.045612\n",
      "Epoch 21/30: Train Loss: 0.0653, Test Loss: 0.0663, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.045612\n",
      "Epoch 22/30: Train Loss: 0.0652, Test Loss: 0.0663, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.045612\n",
      "Epoch 23/30: Train Loss: 0.0652, Test Loss: 0.0663, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.045612\n",
      "Epoch 24/30: Train Loss: 0.0652, Test Loss: 0.0663, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.045612\n",
      "Epoch 25/30: Train Loss: 0.0652, Test Loss: 0.0663, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.041050\n",
      "Epoch 26/30: Train Loss: 0.0652, Test Loss: 0.0663, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.041050\n",
      "Epoch 27/30: Train Loss: 0.0652, Test Loss: 0.0663, Train Acc: 0.9861, Test Acc: 0.9860, LR: 0.041050\n",
      "Early stopping triggered after 27 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13960   226]\n",
      " [  219 17389]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0  0.98455462 0.98406880 0.98431165     14186\n",
      "         1.0  0.98717003 0.98756247 0.98736621     17608\n",
      "\n",
      "    accuracy                      0.98600365     31794\n",
      "   macro avg  0.98586232 0.98581564 0.98583893     31794\n",
      "weighted avg  0.98600307 0.98600365 0.98600331     31794\n",
      "\n",
      "\n",
      "==================== FOLD 3 ====================\n",
      "Epoch 1/30: Train Loss: 0.1209, Test Loss: 0.0756, Train Acc: 0.9762, Test Acc: 0.9866, LR: 0.069519\n",
      "Epoch 2/30: Train Loss: 0.0733, Test Loss: 0.0693, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.069519\n",
      "Epoch 3/30: Train Loss: 0.0697, Test Loss: 0.0673, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.069519\n",
      "Epoch 4/30: Train Loss: 0.0683, Test Loss: 0.0664, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.069519\n",
      "Epoch 5/30: Train Loss: 0.0675, Test Loss: 0.0657, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.062567\n",
      "Epoch 6/30: Train Loss: 0.0670, Test Loss: 0.0654, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.062567\n",
      "Epoch 7/30: Train Loss: 0.0667, Test Loss: 0.0651, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.062567\n",
      "Epoch 8/30: Train Loss: 0.0665, Test Loss: 0.0650, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.062567\n",
      "Epoch 9/30: Train Loss: 0.0663, Test Loss: 0.0649, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.062567\n",
      "Epoch 10/30: Train Loss: 0.0662, Test Loss: 0.0647, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.056311\n",
      "Epoch 11/30: Train Loss: 0.0661, Test Loss: 0.0647, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.056311\n",
      "Epoch 12/30: Train Loss: 0.0660, Test Loss: 0.0647, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.056311\n",
      "Epoch 13/30: Train Loss: 0.0659, Test Loss: 0.0646, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.056311\n",
      "Epoch 14/30: Train Loss: 0.0659, Test Loss: 0.0646, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.056311\n",
      "Epoch 15/30: Train Loss: 0.0659, Test Loss: 0.0646, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.050680\n",
      "Epoch 16/30: Train Loss: 0.0658, Test Loss: 0.0645, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.050680\n",
      "Epoch 17/30: Train Loss: 0.0658, Test Loss: 0.0645, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.050680\n",
      "Epoch 18/30: Train Loss: 0.0657, Test Loss: 0.0645, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.050680\n",
      "Epoch 19/30: Train Loss: 0.0658, Test Loss: 0.0645, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.050680\n",
      "Epoch 20/30: Train Loss: 0.0657, Test Loss: 0.0645, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.045612\n",
      "Epoch 21/30: Train Loss: 0.0657, Test Loss: 0.0645, Train Acc: 0.9860, Test Acc: 0.9866, LR: 0.045612\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[14145   204]\n",
      " [  222 17223]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0  0.98454792 0.98578298 0.98516506     14349\n",
      "         1.0  0.98829403 0.98727429 0.98778390     17445\n",
      "\n",
      "    accuracy                      0.98660125     31794\n",
      "   macro avg  0.98642097 0.98652864 0.98647448     31794\n",
      "weighted avg  0.98660337 0.98660125 0.98660199     31794\n",
      "\n",
      "\n",
      "==================== FOLD 4 ====================\n",
      "Epoch 1/30: Train Loss: 0.1207, Test Loss: 0.0749, Train Acc: 0.9761, Test Acc: 0.9865, LR: 0.069519\n",
      "Epoch 2/30: Train Loss: 0.0734, Test Loss: 0.0685, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.069519\n",
      "Epoch 3/30: Train Loss: 0.0699, Test Loss: 0.0665, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.069519\n",
      "Epoch 4/30: Train Loss: 0.0684, Test Loss: 0.0655, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.069519\n",
      "Epoch 5/30: Train Loss: 0.0677, Test Loss: 0.0649, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.062567\n",
      "Epoch 6/30: Train Loss: 0.0672, Test Loss: 0.0646, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.062567\n",
      "Epoch 7/30: Train Loss: 0.0669, Test Loss: 0.0643, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.062567\n",
      "Epoch 8/30: Train Loss: 0.0667, Test Loss: 0.0642, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.062567\n",
      "Epoch 9/30: Train Loss: 0.0666, Test Loss: 0.0640, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.062567\n",
      "Epoch 10/30: Train Loss: 0.0664, Test Loss: 0.0639, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.056311\n",
      "Epoch 11/30: Train Loss: 0.0663, Test Loss: 0.0639, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.056311\n",
      "Epoch 12/30: Train Loss: 0.0662, Test Loss: 0.0638, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.056311\n",
      "Epoch 13/30: Train Loss: 0.0662, Test Loss: 0.0637, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.056311\n",
      "Epoch 14/30: Train Loss: 0.0661, Test Loss: 0.0637, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.056311\n",
      "Epoch 15/30: Train Loss: 0.0661, Test Loss: 0.0636, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.050680\n",
      "Epoch 16/30: Train Loss: 0.0661, Test Loss: 0.0636, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.050680\n",
      "Epoch 17/30: Train Loss: 0.0660, Test Loss: 0.0636, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.050680\n",
      "Epoch 18/30: Train Loss: 0.0660, Test Loss: 0.0635, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.050680\n",
      "Epoch 19/30: Train Loss: 0.0660, Test Loss: 0.0635, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.050680\n",
      "Epoch 20/30: Train Loss: 0.0660, Test Loss: 0.0635, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.045612\n",
      "Epoch 21/30: Train Loss: 0.0659, Test Loss: 0.0635, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.045612\n",
      "Epoch 22/30: Train Loss: 0.0659, Test Loss: 0.0634, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.045612\n",
      "Epoch 23/30: Train Loss: 0.0659, Test Loss: 0.0635, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.045612\n",
      "Epoch 24/30: Train Loss: 0.0659, Test Loss: 0.0634, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.045612\n",
      "Epoch 25/30: Train Loss: 0.0659, Test Loss: 0.0634, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.041050\n",
      "Epoch 26/30: Train Loss: 0.0659, Test Loss: 0.0634, Train Acc: 0.9860, Test Acc: 0.9865, LR: 0.041050\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m param_distributions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mlogspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m20\u001b[39m),\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatience\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m7\u001b[39m]\n\u001b[0;32m     10\u001b[0m }\n\u001b[1;32m---> 12\u001b[0m hyperparameter_results \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparameter_random_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolds_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfolds_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter_search\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "File \u001b[1;32mc:\\Users\\Devon Javier\\OneDrive\\Desktop\\STINTSY\\INTSY PROJ\\src\\trainEval.py:199\u001b[0m, in \u001b[0;36mhyperparameter_random_search\u001b[1;34m(param_distributions, folds_data, n_iter_search)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28mprint\u001b[39m(params)\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     fold_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfolds_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler_step_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscheduler_step_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler_gamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscheduler_gamma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m     avg_test_accuracy \u001b[38;5;241m=\u001b[39m fold_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_test_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    212\u001b[0m     avg_test_loss \u001b[38;5;241m=\u001b[39m fold_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_test_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Devon Javier\\OneDrive\\Desktop\\STINTSY\\INTSY PROJ\\src\\trainEval.py:125\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(folds_data, learning_rate, batch_size, num_epochs, scheduler_step_size, scheduler_gamma, missing_value, convergence_threshold, patience, weight_decay, optimizer_string, seed)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m#converge criteria\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m--> 125\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     test_loss, test_accuracy, all_y_test, all_predictions \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, criterion)\n\u001b[0;32m    127\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Devon Javier\\OneDrive\\Desktop\\STINTSY\\INTSY PROJ\\src\\trainEval.py:63\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, train_loader, criterion, optimizer)\u001b[0m\n\u001b[0;32m     61\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     62\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m (outputs \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;66;03m# Convert to binary predictions\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     64\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader), correct \u001b[38;5;241m/\u001b[39m total\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_distributions = {\n",
    "    'learning_rate': np.logspace(-4, -1, 20),\n",
    "    'batch_size': [32, 64, 128, 256],\n",
    "    'optimizer': ['sgd', 'adam', 'rmsprop'],\n",
    "    'weight_decay': np.logspace(-5, -2, 10),\n",
    "    'num_epochs': [30, 50, 75, 100],\n",
    "    'scheduler_step_size': [5, 10, 15],\n",
    "    'scheduler_gamma': [0.5, 0.7, 0.9],\n",
    "    'patience': [3, 5, 7]\n",
    "}\n",
    "\n",
    "hyperparameter_results = hyperparameter_random_search(folds_data=folds_data, param_distributions=param_distributions, n_iter_search=50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPT Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##best_params = hyperparameter_results['best_params']\n",
    "##results_summary = hyperparameter_results['summary_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree (DT)\n",
    "\n",
    "Another machine learning model we can use is the decision tree. Decision trees don't necessarily assume a linear relationship between our predictor variables and PUFC11, which could be benificial for finding complex relationships. Rather, they recursively partition our data with rules. Further, while it can get quite complex as it scales, individual nodes are also uncomplicated in how they are interpreted, which is also helpful for understanding the factors that determine whether an individual has worked within the past week.\n",
    "\n",
    "#### Training: DT\n",
    "We can recycle the one-hot encoded data we used in preparation for Logistic Regression and immediately use it for training our decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Train Accuracy: 0.9852, Train Loss: 0.0702, Test Accuracy: 0.9847, Test Loss: 0.0724\n",
      "Fold 1 - Confusion Matrix:\n",
      "[[14020   282]\n",
      " [  206 17286]]\n",
      "\n",
      "Fold 2 - Train Accuracy: 0.9851, Train Loss: 0.0703, Test Accuracy: 0.9849, Test Loss: 0.0712\n",
      "Fold 2 - Confusion Matrix:\n",
      "[[13926   260]\n",
      " [  219 17389]]\n",
      "\n",
      "Fold 3 - Train Accuracy: 0.9850, Train Loss: 0.0706, Test Accuracy: 0.9854, Test Loss: 0.0747\n",
      "Fold 3 - Confusion Matrix:\n",
      "[[14106   243]\n",
      " [  222 17223]]\n",
      "\n",
      "Fold 4 - Train Accuracy: 0.9850, Train Loss: 0.0708, Test Accuracy: 0.9855, Test Loss: 0.0727\n",
      "Fold 4 - Confusion Matrix:\n",
      "[[13951   262]\n",
      " [  200 17380]]\n",
      "\n",
      "Fold 5 - Train Accuracy: 0.9851, Train Loss: 0.0702, Test Accuracy: 0.9850, Test Loss: 0.0724\n",
      "Fold 5 - Confusion Matrix:\n",
      "[[14113   249]\n",
      " [  228 17203]]\n",
      "\n",
      "\n",
      "Aggregated Metrics:\n",
      "Average Train Accuracy: 0.9851\n",
      "Average Train Loss: 0.0704\n",
      "Average Test Accuracy: 0.9851\n",
      "Average Test Loss: 0.0727\n",
      "\n",
      "Overall Confusion Matrix:\n",
      "[[70116  1296]\n",
      " [ 1075 86481]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      0.98      0.98     14362\n",
      "           2       0.99      0.99      0.99     17431\n",
      "\n",
      "    accuracy                           0.98     31793\n",
      "   macro avg       0.98      0.98      0.98     31793\n",
      "weighted avg       0.98      0.98      0.98     31793\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, log_loss, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "accuracies_train = []\n",
    "losses_train = []\n",
    "dt_accuracies_test = []\n",
    "losses_test = []\n",
    "confusion_matrices = [] \n",
    "\n",
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = (\n",
    "        folds_data[i]['X_train'],\n",
    "        folds_data[i]['X_test'],\n",
    "        folds_data[i]['y_train'],\n",
    "        folds_data[i]['y_test'],\n",
    "    )\n",
    "\n",
    "    model = DecisionTreeClassifier(random_state=45, min_impurity_decrease=0.001)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Train predictions and loss\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_train_pred_proba = model.predict_proba(X_train)\n",
    "    accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "    loss_train = log_loss(y_train, y_train_pred_proba)\n",
    "\n",
    "    # Test predictions and loss\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred_proba = model.predict_proba(X_test)\n",
    "    accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "    loss_test = log_loss(y_test, y_test_pred_proba)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    confusion_matrices.append(cm)\n",
    "\n",
    "    accuracies_train.append(accuracy_train)\n",
    "    losses_train.append(loss_train)\n",
    "    dt_accuracies_test.append(accuracy_test)\n",
    "    losses_test.append(loss_test)\n",
    "\n",
    "    print(f\"Fold {i+1} - Train Accuracy: {accuracy_train:.4f}, Train Loss: {loss_train:.4f}, Test Accuracy: {accuracy_test:.4f}, Test Loss: {loss_test:.4f}\")\n",
    "    print(f\"Fold {i+1} - Confusion Matrix:\\n{cm}\\n\")\n",
    "\n",
    "avg_accuracy_train = np.mean(accuracies_train)\n",
    "avg_loss_train = np.mean(losses_train)\n",
    "avg_accuracy_test = np.mean(dt_accuracies_test)\n",
    "avg_loss_test = np.mean(losses_test)\n",
    "\n",
    "print(f\"\\nAggregated Metrics:\")\n",
    "print(f\"Average Train Accuracy: {avg_accuracy_train:.4f}\")\n",
    "print(f\"Average Train Loss: {avg_loss_train:.4f}\")\n",
    "print(f\"Average Test Accuracy: {avg_accuracy_test:.4f}\")\n",
    "print(f\"Average Test Loss: {avg_loss_test:.4f}\")\n",
    "\n",
    "print(\"\\nOverall Confusion Matrix:\")\n",
    "print(sum(confusion_matrices))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error: DT\n",
    "Our model is also performing well, and with each confusion metric no less than 0.98. As this is a decision tree, a model usually prone to overfitting, we do see minimal overfitting, seeing as our train loss is higher than our test loss by 0.0023, though this is arguably a negligible gap.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning: DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to our methodology with Logistic Regression, we will implement random search for our tuning as a compromise to exhaustively searching through each hyperparameter combination. This time, we will explicitly define certain thresholds and measures. \n",
    "\n",
    "Our previous two hyperparameters, max_depth and minimum_impurity_decrease, will now range fom 30-60 and 0.001 to 0.1 respectively, adding variability to our parameters despite minimal signs of overfitting. We will allow the min_samples_leaf to enforce generalization or allow leaf nodes with one sample. \n",
    "\n",
    "Of note, we will add cost-complexity pruning as a form of regularizaton for our decision trees. We will also be exploring both gini and entropy as criteria, as the former minimizes chances of misclassification while the latter measures information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error: DT\n",
    "Our model is also performing well, and with each confusion metric no less than 0.98. As this is a decision tree, a model usually prone to overfitting, we do see minimal overfitting, seeing as our train loss is higher than our test loss by 0.0023, though this is arguably a negligible gap.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fold 1 - Best Parameters: {'min_samples_split': 5, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.001, 'max_depth': 50, 'criterion': 'entropy', 'ccp_alpha': np.float64(0.0001)}\n",
      "Fold 1 - Accuracy: 0.9857, Precision: 0.9856, Recall: 0.9854, F1-score: 0.9855, Loss: 0.0689\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fold 2 - Best Parameters: {'min_samples_split': 5, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.001, 'max_depth': 50, 'criterion': 'entropy', 'ccp_alpha': np.float64(0.0001)}\n",
      "Fold 2 - Accuracy: 0.9860, Precision: 0.9859, Recall: 0.9858, F1-score: 0.9858, Loss: 0.0673\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fold 3 - Best Parameters: {'min_samples_split': 5, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.001, 'max_depth': 50, 'criterion': 'entropy', 'ccp_alpha': np.float64(0.0001)}\n",
      "Fold 3 - Accuracy: 0.9863, Precision: 0.9861, Recall: 0.9861, F1-score: 0.9861, Loss: 0.0715\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 35\u001b[0m\n\u001b[0;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m45\u001b[39m)\n\u001b[0;32m     32\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m     33\u001b[0m     model, p_grid, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m45\u001b[39m\n\u001b[0;32m     34\u001b[0m )\n\u001b[1;32m---> 35\u001b[0m \u001b[43mrandom_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m best_params_list\u001b[38;5;241m.\u001b[39mappend(random_search\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[0;32m     39\u001b[0m accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy_score(y_test, random_search\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mpredict(X_test)))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:1951\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1951\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1953\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1954\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1955\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    967\u001b[0m         )\n\u001b[0;32m    968\u001b[0m     )\n\u001b[1;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "\n",
    "p_grid = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [30, 45, 50, 60],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"min_impurity_decrease\": [0.001, 0.005, 0.01], ## pre-pruning\n",
    "    \"ccp_alpha\": np.logspace(-4, 0, 10) ## post-pruning\n",
    "}\n",
    "\n",
    "best_params_list = []\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "losses = []\n",
    "\n",
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = (\n",
    "        folds_data[i]['X_train'],\n",
    "        folds_data[i]['X_test'],\n",
    "        folds_data[i]['y_train'],\n",
    "        folds_data[i]['y_test'],\n",
    "    )\n",
    "\n",
    "    model = DecisionTreeClassifier(random_state=45)\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        model, p_grid, n_iter=20, cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=1, random_state=45\n",
    "    )\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params_list.append(random_search.best_params_)\n",
    "\n",
    "    accuracies.append(accuracy_score(y_test, random_search.best_estimator_.predict(X_test)))\n",
    "    report = classification_report(y_test, random_search.best_estimator_.predict(X_test), output_dict=True, zero_division=0)\n",
    "    losses.append(log_loss(y_test, random_search.best_estimator_.predict_proba(X_test)))\n",
    "    precisions.append(report['macro avg']['precision'])\n",
    "    recalls.append(report['macro avg']['recall'])\n",
    "    f1_scores.append(report['macro avg']['f1-score'])\n",
    "\n",
    "    print(f\"Fold {i+1} - Best Parameters: {random_search.best_params_}\")\n",
    "    print(f\"Fold {i+1} - Accuracy: {accuracies[-1]:.4f}, Precision: {precisions[-1]:.4f}, Recall: {recalls[-1]:.4f}, F1-score: {f1_scores[-1]:.4f}, Loss: {losses[-1]:.4f}\")\n",
    "\n",
    "\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_precision = np.mean(precisions)\n",
    "avg_recall = np.mean(recalls)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "avg_loss = np.mean(losses)\n",
    "\n",
    "print(f\"\\nAggregated Metrics:\")\n",
    "print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average F1-score: {avg_f1:.4f}\")\n",
    "print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "print(\"\\nBest Parameters for each fold:\")\n",
    "for i, params in enumerate(best_params_list):\n",
    "    print(f\"Fold {i+1}: {params}\")\n",
    "\n",
    "\n",
    "best_fold_index = np.argmax(accuracies)\n",
    "best_fold_params = best_params_list[best_fold_index]\n",
    "print(\"\\nBest Performing Fold Parameters:\", best_fold_params)\n",
    "\n",
    "\n",
    "best_model = DecisionTreeClassifier(random_state=45, **best_fold_params)\n",
    "\n",
    "\n",
    "all_X_train = np.concatenate([folds_data[i]['X_train'] for i in range(5)])\n",
    "all_y_train = np.concatenate([folds_data[i]['y_train'] for i in range(5)])\n",
    "\n",
    "best_model.fit(all_X_train, all_y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Devon Javier\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 135 features, but DecisionTreeClassifier is expecting 160 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m train_preds \u001b[38;5;241m=\u001b[39m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m accuracy_score(y_train, train_preds)\n\u001b[0;32m      8\u001b[0m test_preds \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\tree\\_classes.py:530\u001b[0m, in \u001b[0;36mBaseDecisionTree.predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict class or regression value for X.\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \n\u001b[0;32m    509\u001b[0m \u001b[38;5;124;03mFor a classification model, the predicted class for each sample in X is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;124;03m    The predicted classes, or the predict values.\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    529\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 530\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m    532\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\tree\\_classes.py:489\u001b[0m, in \u001b[0;36mBaseDecisionTree._validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m     ensure_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    498\u001b[0m     X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc\n\u001b[0;32m    499\u001b[0m ):\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2965\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m-> 2965\u001b[0m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2829\u001b[0m, in \u001b[0;36m_check_n_features\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m-> 2829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2830\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2831\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2832\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 135 features, but DecisionTreeClassifier is expecting 160 features as input."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, log_loss, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "train_preds = best_model.predict(X_train)\n",
    "train_acc = accuracy_score(y_train, train_preds)\n",
    "\n",
    "test_preds = best_model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, test_preds)\n",
    "\n",
    "train_probs = best_model.predict_proba(X_train)\n",
    "test_probs = best_model.predict_proba(X_test)\n",
    "train_loss = log_loss(y_train, train_probs)\n",
    "test_loss = log_loss(y_test, test_probs)\n",
    "\n",
    "cm = confusion_matrix(y_test, test_preds)  \n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, \n",
    "            xticklabels=[\"Predicted Negative\", \"Predicted Positive\"], \n",
    "            yticklabels=[\"Actual Negative\", \"Actual Positive\"])\n",
    "\n",
    "\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(cm)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "print(f\"Training Log Loss: {train_loss:.4f}\")\n",
    "print(f\"Test Log Loss: {test_loss:.4f}\")\n",
    "\n",
    "\n",
    "train_precision = precision_score(y_train, train_preds, zero_division=0)\n",
    "test_precision = precision_score(y_test, test_preds, zero_division=0)\n",
    "\n",
    "train_recall = recall_score(y_train, train_preds, zero_division=0)\n",
    "test_recall = recall_score(y_test, test_preds, zero_division=0)\n",
    "\n",
    "train_f1 = f1_score(y_train, train_preds, zero_division=0)\n",
    "test_f1 = f1_score(y_test, test_preds, zero_division=0)\n",
    "\n",
    "print(f\"Training Precision: {train_precision:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "\n",
    "print(f\"Training Recall: {train_recall:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "print(f\"Training F1-score: {train_f1:.4f}\")\n",
    "print(f\"Test F1-score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "Fold 1 - Train Accuracy: 0.9862, Train Loss: 0.4597, Test Accuracy: 0.9855, Test Loss: 0.4779\n",
      "Fold 1 - Confusion Matrix:\n",
      "[[14051   251]\n",
      " [  209 17283]]\n",
      "\n",
      "fold 2\n",
      "Fold 2 - Train Accuracy: 0.9854, Train Loss: 0.4657, Test Accuracy: 0.9848, Test Loss: 0.4808\n",
      "Fold 2 - Confusion Matrix:\n",
      "[[13962   224]\n",
      " [  260 17348]]\n",
      "\n",
      "fold 3\n",
      "Fold 3 - Train Accuracy: 0.9860, Train Loss: 0.4627, Test Accuracy: 0.9863, Test Loss: 0.4518\n",
      "Fold 3 - Confusion Matrix:\n",
      "[[14142   207]\n",
      " [  228 17217]]\n",
      "\n",
      "fold 4\n",
      "Fold 4 - Train Accuracy: 0.9853, Train Loss: 0.4610, Test Accuracy: 0.9854, Test Loss: 0.4617\n",
      "Fold 4 - Confusion Matrix:\n",
      "[[13986   227]\n",
      " [  236 17344]]\n",
      "\n",
      "fold 5\n",
      "Fold 5 - Train Accuracy: 0.9862, Train Loss: 0.4574, Test Accuracy: 0.9855, Test Loss: 0.4924\n",
      "Fold 5 - Confusion Matrix:\n",
      "[[14135   227]\n",
      " [  233 17198]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#initial k value, optimal value will be determined later\n",
    "k = 5\n",
    "\n",
    "accuracies_train = []\n",
    "losses_train = []\n",
    "knn_accuracies_test = []\n",
    "losses_test = []\n",
    "confusion_matrices = [] \n",
    "\n",
    "for i, fold in enumerate(folds_data):\n",
    "    print(f\"fold {i+1}\")\n",
    "\n",
    "    X_train = fold['X_train']\n",
    "    y_train = fold['y_train']\n",
    "    X_test = fold['X_test']\n",
    "    y_test = fold['y_test']\n",
    "\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "    knn_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = knn_model.predict(X_test)\n",
    "\n",
    "    y_train_pred = knn_model.predict(X_train)\n",
    "    y_train_pred_proba = knn_model.predict_proba(X_train)\n",
    "    accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "    loss_train = log_loss(y_train, y_train_pred_proba)\n",
    "\n",
    "    y_test_pred = knn_model.predict(X_test)\n",
    "    y_test_pred_proba = knn_model.predict_proba(X_test)\n",
    "    accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "    loss_test = log_loss(y_test, y_test_pred_proba)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "    accuracies_train.append(accuracy_train)\n",
    "    losses_train.append(loss_train)\n",
    "    knn_accuracies_test.append(accuracy_test)\n",
    "    losses_test.append(loss_test)\n",
    "\n",
    "    ## perf metrics\n",
    "    print(f\"Fold {i+1} - Train Accuracy: {accuracy_train:.4f}, Train Loss: {loss_train:.4f}, Test Accuracy: {accuracy_test:.4f}, Test Loss: {loss_test:.4f}\")\n",
    "    print(f\"Fold {i+1} - Confusion Matrix:\\n{cm}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Optimal K value (hpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best k: 5\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_neighbors': range(1, 31)} \n",
    "\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(weights='distance'), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"best k:\", grid_search.best_params_['n_neighbors'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14135   227]\n",
      " [  233 17198]]\n",
      "acc:  0.9855314062844023\n"
     ]
    }
   ],
   "source": [
    "knn_cm = confusion_matrix(y_test, y_pred)\n",
    "print(knn_cm)\n",
    "knn_acc = accuracy_score(y_test, y_pred)\n",
    "print(\"acc: \", knn_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(lfs_data[target_col], bins=30, kde=True)\n",
    "plt.title(\"Histogram of Target Variable\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LR Accuracies: [0.985657671258728, 0.986003648487136, 0.9866012455180223, 0.9865379171515742, 0.985720127071997]\n",
      " DT Accuracies: [0.98465119 0.98493426 0.9853746  0.9854685  0.9849967 ]\n",
      " KNN Accuracies: [0.98553186 0.984777   0.98631817 0.98543705 0.98553141]\n",
      "Friedman Test: Chi-square statistic = 7.6000000000000085 , p-value = 0.0223707718561655\n",
      "\n",
      "Significant difference detected! Running McNemar’s test...\n",
      "\n",
      "LR vs DT: McNemar's p-value = 1.0\n",
      "No significant difference between LR and DT.\n",
      "\n",
      "LR vs KNN: McNemar's p-value = 1.0\n",
      "No significant difference between LR and KNN.\n",
      "\n",
      "DT vs KNN: McNemar's p-value = 1.0\n",
      "No significant difference between DT and KNN.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import friedmanchisquare, wilcoxon\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "print(\" LR Accuracies:\", lr_accuracies_test)\n",
    "print(\" DT Accuracies:\", dt_accuracies_test)\n",
    "print(\" KNN Accuracies:\", knn_accuracies_test)\n",
    "\n",
    "dt_accuracies_test = np.array(dt_accuracies_test)\n",
    "knn_accuracies_test = np.array(knn_accuracies_test) \n",
    "rf_accuracies_test = np.array(lr_accuracies_test) \n",
    "\n",
    "data = np.vstack((lr_accuracies_test, dt_accuracies_test, knn_accuracies_test)).T\n",
    "\n",
    "f_stat, p_value = friedmanchisquare(lr_accuracies_test, dt_accuracies_test, knn_accuracies_test)\n",
    "print(\"Friedman Test: Chi-square statistic =\", f_stat, \", p-value =\", p_value)\n",
    "\n",
    "\n",
    "## idk what im doing\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"\\nSignificant difference detected! Running McNemar’s test...\\n\")\n",
    "\n",
    "\n",
    "    lr_preds = np.array([1, 1, 0, 1, 1])  \n",
    "    dt_preds = np.array([1, 1, 1, 0, 1])  \n",
    "    knn_preds = np.array([0, 1, 1, 1, 1])  \n",
    "\n",
    "    def run_mcnemar(model1, model2, name1, name2):\n",
    "        \"\"\"Runs McNemar's test between two classifiers.\"\"\"\n",
    "        table = np.zeros((2, 2))\n",
    "        for i in range(len(model1)):\n",
    "            table[model1[i], model2[i]] += 1  \n",
    "\n",
    "        result = mcnemar(table, exact=True)\n",
    "        print(f\"{name1} vs {name2}: McNemar's p-value = {result.pvalue}\")\n",
    "\n",
    "        if result.pvalue < 0.05:\n",
    "            print(f\"Significant difference between {name1} and {name2}!\\n\")\n",
    "        else:\n",
    "            print(f\"No significant difference between {name1} and {name2}.\\n\")\n",
    "\n",
    "    run_mcnemar(lr_preds, dt_preds, \"LR\", \"DT\")\n",
    "    run_mcnemar(lr_preds, knn_preds, \"LR\", \"KNN\")\n",
    "    run_mcnemar(dt_preds, knn_preds, \"DT\", \"KNN\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo significant difference detected. McNemar's test is unnecessary.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
