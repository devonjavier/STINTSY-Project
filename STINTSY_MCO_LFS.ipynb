{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Introduction ##\n",
    "\n",
    "In this notebook, the dataset to be processed is the Labor Force Survey conducted April 2016 and retrieved through Philippine Statistics Authority database. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6.0, 6.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# autoreload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importing LFS PUF April 2016.CSV</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "try:\n",
    "    lfs_data = pd.read_csv(\"LFS PUF April 2016.CSV\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: CSV file not found. Please make sure the file exists in the correct directory or provide the correct path.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Information</h1>\n",
    "\n",
    "Let's get an overview of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Of interest to us, there are:\n",
    "<ul><li>1 contains float values, </li>\n",
    "<li>14 contain integer values, and </li>\n",
    "<li><b>35 are object values</b>.</li></ul>\n",
    "<br>\n",
    "We can then infer that we must first <b>format the 35 columns</b> that have object values before even doing any processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also apply the unique() function to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_data.apply(lambda x: x.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Considering our dataset has 18,000 entries, features with particularly low numbers stand out as questions that have clear, defined choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicates here, and therefore no cleaning need follow in this regard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset seems to contain null values in the form of whitespaces. Let's count those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_null = lfs_data.apply(lambda col: col.str.isspace().sum() if col.dtype == 'object' else 0)\n",
    "\n",
    "print(\"Number Empty Cells:\")\n",
    "print(has_null[has_null > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "And standardize, replacing these whitespace values with NaN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_data.replace(r\"^\\s+$\", np.nan, regex=True, inplace=True)\n",
    "nan_counts_per_column = lfs_data.isna().sum()\n",
    "print(nan_counts_per_column[nan_counts_per_column > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Preprocessing and Data Cleaning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PUFC06_MSTAT</h2>\n",
    "Predictors:\n",
    "<ul><li>PUFC05_AGE</li>\n",
    "<li>PUFC04_SEX </li>\n",
    "<li>PUFC03_REL </li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "columns = [\"PUFC06_MSTAT\", \"PUFC05_AGE\", \"PUFC04_SEX\", \"PUFC03_REL\"]\n",
    "lfs_data_PUFC06_MSTAT = lfs_data[columns]\n",
    "pd.get_dummies(lfs_data_PUFC06_MSTAT, columns=[\"PUFC04_SEX\", \"PUFC03_REL\"])\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "# lfs_data_PUFC06_MSTAT = pd.DataFrame(imputer.fit_transform(lfs_data_PUFC06_MSTAT), columns=lfs_data_PUFC06_MSTAT.columns)\n",
    "# lfs_data_PUFC06_MSTAT[\"PUFC06_MSTAT\"].round().astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_data_PUFC06_MSTAT = pd.DataFrame(imputer.fit_transform(lfs_data_PUFC06_MSTAT), columns=lfs_data_PUFC06_MSTAT.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_data_PUFC06_MSTAT[\"PUFC06_MSTAT\"].round().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PUFC08_CURSCH</h2>\n",
    "Is the person currently attending school?\n",
    "\n",
    "TODO: \n",
    "since current variables are just 1 and 2<br>\n",
    "where 1 or 2 represent \"elementary education\" and \"secondary and tertiary education\" accomplishment<br>\n",
    "<br>\n",
    "and so we will replace all null values with 0<br>\n",
    "to represent having NOT finished elementary, secondary, or tertiary.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PUFC31_FLWRK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PUFC32_JOBSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PUFC33_WEEKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PUFC35_LTLOOKW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PUFC36_AVAIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PUFC37_WILLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>One Hot Encoding</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idk pa what columns to hot encoding \n",
    "# df = pd.get_dummies(df, drop_first=True)  # One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Feature Selection</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idk pa rin what to do here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary \n",
    "\n",
    "TODO:<br>\n",
    "convert the columns that can be classified in a binary manner to 1s and 0s<br>\n",
    "ie. employment status: instead of \"employed\" and \"unemployed\"<br>\n",
    "convert to 1 and 0<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>kNN</h1>\n",
    "\n",
    "Employability or Job Prediction (kNN) (??)\n",
    "\n",
    "Rural vs Urban Workforce Disparities (kNN)\n",
    "<ul><li>group up regions that have similar labor market characteristics, challenges, and/or opportunities</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 1. Import the CSV file using pandas\n",
    "try:\n",
    "    lfs_data = pd.read_csv(\"LFS_PUF_April_2016.CSV\")  # Replace with the actual file name/path\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: CSV file not found. Please make sure the file exists in the correct directory or provide the correct path.\")\n",
    "    # Handle the error gracefully, e.g., exit the program or prompt the user for the correct path.\n",
    "    exit()  # Or another appropriate error handling\n",
    "\n",
    "# 2. Data Cleaning and Preprocessing (Crucial for kNN)\n",
    "\n",
    "# --- A. Handle Missing Values ---\n",
    "# kNN is sensitive to missing values.  You'll need to decide how to handle them.  Common options:\n",
    "\n",
    "# Option 1: Drop rows with any missing values (if the number of missing values is small)\n",
    "lfs_data.dropna(inplace=True)  # This modifies the DataFrame in place\n",
    "\n",
    "# Option 2: Impute missing values (more common and often better)\n",
    "#   - Numerical features: Use mean, median, or more advanced imputation (e.g., KNN imputation)\n",
    "#     Example using median:\n",
    "#     for col in lfs_data.select_dtypes(include=['number']).columns:\n",
    "#         lfs_data[col].fillna(lfs_data[col].median(), inplace=True)\n",
    "\n",
    "#   - Categorical features: Use mode (most frequent value)\n",
    "#     for col in lfs_data.select_dtypes(exclude=['number']).columns:\n",
    "#         lfs_data[col].fillna(lfs_data[col].mode()[0], inplace=True) # mode() returns a series, take the first element\n",
    "\n",
    "\n",
    "# --- B. Feature Selection/Engineering ---\n",
    "# Identify your target variable (the one you want to predict) and features (the ones you'll use for prediction).\n",
    "# Example (replace 'TARGET_COLUMN' and 'FEATURE_COLUMNS' with your actual column names):\n",
    "TARGET_COLUMN = 'EmploymentStatus' # Example - replace with your actual target variable\n",
    "FEATURE_COLUMNS = ['Age', 'EducationLevel', 'Occupation'] # Example - replace with your feature columns\n",
    "\n",
    "y = lfs_data[TARGET_COLUMN]  # Target variable\n",
    "X = lfs_data[FEATURE_COLUMNS] # Features\n",
    "\n",
    "# --- C. Encode Categorical Features ---\n",
    "# kNN works with numerical data. Convert categorical features to numerical using one-hot encoding or label encoding.\n",
    "\n",
    "X = pd.get_dummies(X) # One-hot encoding (often preferred for kNN)\n",
    "# OR\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# for col in X.select_dtypes(exclude=['number']).columns:\n",
    "#     X[col] = le.fit_transform(X[col])\n",
    "\n",
    "# --- D. Feature Scaling (Very Important for kNN) ---\n",
    "# kNN is distance-based, so features with larger values can dominate. Scale your features.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X) # Fit and transform the features\n",
    "\n",
    "# 3. Split Data into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)  # 80% train, 20% test\n",
    "\n",
    "# 4. Train the kNN Classifier\n",
    "k = 5  # Choose an appropriate value for k (number of neighbors) - often needs tuning\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# 5. Make Predictions\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# 6. Evaluate the Model\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "# --- Important Notes ---\n",
    "\n",
    "# * **File Path:** Double-check the path to your CSV file.  If it's not in the same directory as your notebook, provide the full path.\n",
    "# * **Data Exploration:** Before preprocessing, explore your data: `lfs_data.head()`, `lfs_data.info()`, `lfs_data.describe()`. This will help you understand the data types, missing values, and potential issues.\n",
    "# * **Feature Engineering:**  The choice of features and how you engineer them is *crucial* for model performance.\n",
    "# * **k Value Tuning:** Experiment with different values of `k` to find the optimal one. You can use techniques like cross-validation.\n",
    "# * **Handling Imbalanced Datasets:** If your target variable has imbalanced classes (e.g., many more examples of one class than another), consider techniques like oversampling or undersampling.\n",
    "# * **Computational Resources:** The LFS PUF is a large dataset. Be mindful of your computer's memory.  You might need to process the data in chunks if you run into memory issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "\n",
    "Employability (Binary Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "model = SGDClassifier(\n",
    "    loss='log_loss',\n",
    "    eta0=0.001,\n",
    "    max_iter=200,\n",
    "    learning_rate='constant',\n",
    "    random_state=1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>try</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Load the data\n",
    "try:\n",
    "    lfs_data = pd.read_csv(\"LFS PUF April 2016.CSV\")\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: CSV file not found. Please ensure the file is in the correct directory or provide the correct path.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Cleaning and Preprocessing\n",
    "\n",
    "# a. Handle Missing Values (Important!)\n",
    "# Explore missing data:\n",
    "print(lfs_data.isnull().sum()) # Check for missing values in each column\n",
    "# Strategies for handling missing data (choose one or combine as appropriate):\n",
    "# 1. Drop rows with many missing values (if applicable).\n",
    "# 2. Impute missing numerical values (e.g., mean, median).\n",
    "# 3. Impute missing categorical values (e.g., mode, or create a \"missing\" category).\n",
    "# Example imputation (replace with more suitable strategy as needed):\n",
    "# lfs_data['PUFC25_PBASIC'].fillna(lfs_data['PUFC25_PBASIC'].median(), inplace=True) # Impute with median for basic pay\n",
    "# lfs_data['PUFC07_GRADE'].fillna(\"Unknown\", inplace=True) # Impute with \"Unknown\" for grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i think we can drop PUFSVYMO Survey month, PUFSVYYR Survey year\n",
    "\n",
    "renamed_fucking_columns = {\n",
    "    'PUFREG': 'Region',\n",
    "    'PUFPRV' : 'Province code',\n",
    "    'PUFPRRCD' : 'Province recode',\n",
    "    'PUFHHNUM' : 'Household unique sequential number',\n",
    "    'PUFURB2K10' : 'Urban / Rural',\n",
    "    'PUFPWGTFIN' : 'Final weight',\n",
    "    'PUFSVYMO' : 'Survey month',\n",
    "    'PUFSVYYR' : 'Survey year',\n",
    "    'PUFPSU' : 'PSU number',\n",
    "    'PUFRPL' : 'Replicate',\n",
    "    'PUFHHSIZE' : 'Number of household members',\n",
    "    'PUFC01_LNO' : 'Line number used to identify each member of the household in the survey',\n",
    "    'PUFC03_REL' : 'Relationship of the person to the household head',\n",
    "    'PUFC04_SEX' : 'Sex of the person',\n",
    "    'PUFC05_AGE' : 'Age of the person since last birthday',\n",
    "    'PUFC06_MSTAT' : 'Marital status of the person since last birthday',\n",
    "    'PUFC07_GRADE' : 'Highest grade completed of the person',\n",
    "    'PUFC08_CURSCH' : 'Is the person currently attending school?',\n",
    "    'PUFC09_GRADTECH' : 'Is the person a graduate of a technical / vocational course?',\n",
    "    'PUFC10_CONWR' : 'Category of OFW',\n",
    "    'PUFC11_WORK' : 'Did the person do any work for at least one house during the past week?',\n",
    "    'PUFC12_JOB' : 'Although the person did not work last week, did the person have a job or business during the past week?',\n",
    "    'PUFC14_PROCC' : 'What is the primary occupation of the person during the past week?',\n",
    "    'PUFC16_PKB' : 'Kind of business or industry of the person',\n",
    "    'PUFC17_NATEM' : 'Nature of employment of the person.',\n",
    "        # This refers to the permanence or regularity or seasonality with which a particular work or job/business is being pursued.\n",
    "    'PUFC18_PNWHRS' : 'Normal working hours per day',\n",
    "        # Normal working hours worked per day is the usual or prescribed working hours of a person in his primary job/business, which is, considered a full day's work.\n",
    "    'PUFC19_PHOURS' : 'Total number of hours worked during the past week',\n",
    "        # The actual number of hours worked by a person in his primary job that he held during the past week or in his other job(s)/business if there are or if there is any.\n",
    "        # It includes the duration or the period the person was occupied in his work, including overtime, but excluding hours paid but not worked. \n",
    "        # For wage and salary earners, it includes time worked without compensation in connection with their occupations, \n",
    "        # such as the time a teacher spends at home preparing for the forthcoming lectures. \n",
    "        # For own account workers, it includes the time spent in the shop, business or office, even if no sale or transaction has taken place.\n",
    "    'PUFC20_PWMORE' : 'Do you want more hours of work during the past week?',\n",
    "    'PUFC21_PLADDW' : 'Did the person look for additional work during the past week?',\n",
    "    'PUFC22_PFWRK' : \"Was this the person's first time to do any work?\",\n",
    "        # This question determines whether a person is a “new entrant” to the labor force. \n",
    "        # A person is a new entrant if it is his first time to do any work.\n",
    "        # A person is considered to have worked only for the first time if he started working only during the current survey period.\n",
    "        # Current survey period refers to April 1 - 30 for this survey round\n",
    "    'PUFC23_PCLASS' : 'Class of worker for primary occupation',\n",
    "        # Class of worker is the relationship of the worker to the establishment where he works.\n",
    "    'PUFC24_PBASIS' : 'Basis of payment for primary occupation',\n",
    "    'PUFC25_PBASIC' : 'Basic pay per day for primary occupation',\n",
    "        # Basic pay is the pay for normal time, prior to deductions of social security contributions, withholding taxes, etc. \n",
    "        # It excludes allowances, bonuses, commissions, overtime pay, benefits in kind, etc. \n",
    "        # This is also called basic wage.\n",
    "    'PUFC26_OJOB' : 'Did the person have other job or business during the past week?',\n",
    "    'PUFC27_NJOBS' : 'Number of jobs the person had during the past week',\n",
    "    'PUFC28_THOURS' : 'Total number of hours worked by the person for all his jobs during the past week',\n",
    "    'PUFC29_WWM48H' : 'Main reason for not working more than 48 hours in the past week',\n",
    "    'PUFC30_LOOKW' : 'Did the person look for work or try to establish a business in the past week?',\n",
    "    'PUFC31_FLWRK' : \"Was it the person's first time looking for work or trying to establish a business?\",\n",
    "    'PUFC32_JOBSM' : 'Job search method',\n",
    "        # What has the person been doing to find work?\n",
    "    'PUFC33_WEEKS' : 'Number of weeks spent in looking for work',\n",
    "        # How many weeks has the person been looking for work?',\n",
    "    'PUFC34_WYNOT' : 'Reason for not looking for work Why did the person not look for work?',\n",
    "    'PUFC35_LTLOOKW' : 'When was the last time the person looked for work?',\n",
    "    'PUFC36_AVAIL' : 'Had opportunity for work existed last week or within two weeks, would the person have been available?',\n",
    "    'PUFC37_WILLING' : 'Is the person willing to take up work in the past week or within 2 weeks?',\n",
    "    'PUFC38_PREVJOB' : 'Has the person worked at any time before?',\n",
    "    'PUFC40_POCC' : 'What was the person’s last occupation?',\n",
    "    'PUFC41_WQTR' : 'Did the person work at all or had a job or business during the past quarter?',\n",
    "    'PUFC43_QKB' : 'Kind of business for the past quarter',\n",
    "    'PUFNEWEMPSTAT' : 'New Employment Criteria'\n",
    "}\n",
    "pd.set_option('display.max_columns', None)\n",
    "lfs_data.rename(columns=renamed_fucking_columns, inplace=True)\n",
    "\n",
    "lfs_data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Feature Selection (Crucial for a good model)\n",
    "# Identify features relevant to employability.  Consider these factors:\n",
    "# * Demographic features (age, sex, marital status, education)\n",
    "# * Work experience (previous jobs, hours worked)\n",
    "# * Job search activity (looking for work, methods used)\n",
    "# * Availability and willingness to work\n",
    "# * Location (region, urban/rural)\n",
    "\n",
    "# Example: Select some potentially relevant features (you'll likely want to refine this):\n",
    "selected_features = ['PUFC05_AGE', 'PUFC04_SEX', 'PUFC07_GRADE', 'PUFC11_WORK', 'PUFC14_PROCC', 'PUFC17_NATEM', 'PUFC23_PCLASS', 'PUFC30_LOOKW', 'PUFC36_AVAIL', 'PUFC37_WILLING', 'PUFNEWEMPSTAT']  # Add more!\n",
    "lfs_data = lfs_data[selected_features]\n",
    "\n",
    "# c. Encode Categorical Variables\n",
    "# Logistic regression works with numerical data. Convert categorical features:\n",
    "lfs_data = pd.get_dummies(lfs_data, columns=['PUFC04_SEX', 'PUFC07_GRADE', 'PUFC11_WORK', 'PUFC14_PROCC', 'PUFC17_NATEM', 'PUFC23_PCLASS', 'PUFC30_LOOKW', 'PUFC36_AVAIL', 'PUFC37_WILLING']) # One-hot encoding\n",
    "\n",
    "# d. Define Target Variable (Employability)\n",
    "# You'll need to define what \"employable\" means in your context.\n",
    "# Example: If PUFNEWEMPSTAT indicates employment status, you might use it directly.\n",
    "# Or, you might create a new target variable based on a combination of factors.\n",
    "# Example (using PUFNEWEMPSTAT directly as a binary indicator - Adapt as needed):\n",
    "lfs_data['employable'] = lfs_data['PUFNEWEMPSTAT'].apply(lambda x: 1 if x in [1, 2, 3] else 0) # Example: 1 if employed, 0 if not.  Adjust based on your data.\n",
    "lfs_data.drop('PUFNEWEMPSTAT', axis=1, inplace=True) # Remove the original employment status column if you created a new 'employable' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Model Training\n",
    "X = lfs_data.drop('employable', axis=1)  # Features\n",
    "y = lfs_data['employable']  # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Split data\n",
    "\n",
    "model = LogisticRegression(max_iter=1000) # Increase max_iter if needed.\n",
    "model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Feature Importance (Optional but helpful)\n",
    "# Logistic regression can provide some insight into feature importance (coefficients):\n",
    "coefficients = model.coef_[0]\n",
    "feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': coefficients})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "\n",
    "# Suggestions for an Outstanding Model:\n",
    "\n",
    "# * Thorough Data Cleaning: Handle missing values strategically.  Don't just drop them blindly. Imputation is often better.\n",
    "# * Feature Engineering: Create new features from existing ones.  For example, combine education and work experience.\n",
    "# * Feature Selection: Carefully choose the most relevant features. Use domain knowledge, statistical tests, or feature selection techniques (e.g., recursive feature elimination).\n",
    "# * Model Selection: Don't be limited to logistic regression. Explore other models like Random Forest, Gradient Boosting, or Support Vector Machines.\n",
    "# * Hyperparameter Tuning: Optimize the model's parameters using techniques like GridSearchCV or RandomizedSearchCV.\n",
    "# * Cross-Validation: Use techniques like k-fold cross-validation to get a more robust estimate of model performance.\n",
    "# * Address Class Imbalance (if present): If your dataset has a significantly unequal number of \"employable\" and \"not employable\" individuals, consider techniques like oversampling or undersampling.\n",
    "# * Domain Expertise:  The most important thing! Work with people who understand the Philippine labor market. Their insights will be invaluable for feature selection, defining \"employability,\" and interpreting the model's results.\n",
    "# * Explainability: Consider using techniques to make your model more interpretable. This is important for understanding why the model is making certain predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
