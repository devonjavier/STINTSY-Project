{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Introduction ##\n",
    "\n",
    "In this notebook, the dataset to be processed is the Labor Force Survey conducted April 2016 and retrieved through Philippine Statistics Authority database. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6.0, 6.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# autoreload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importing LFS PUF April 2016.CSV</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    lfs_data = pd.read_csv(\"src/data/LFS PUF April 2016.CSV\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: CSV file not found. Please make sure the file exists in the correct directory or provide the correct path.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Information, Pre-Processing, and Cleaning</h1>\n",
    "\n",
    "Let's get an overview of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Of interest to us, there are:\n",
    "<ul><li>1 contains float values, </li>\n",
    "<li>14 contain integer values, and </li>\n",
    "<li><b>35 are object values</b>.</li></ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's check for duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicates here, and therefore no cleaning need follow in this regard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset seems to contain null values in the form of whitespaces. Let's count those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_null = lfs_data.apply(lambda col: col.str.isspace().sum() if col.dtype == 'object' else 0)\n",
    "\n",
    "print(\"Number Empty Cells:\")\n",
    "print(has_null[has_null > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "And standardize, replacing these whitespace values with -1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_data.replace(r\"^\\s+$\", -1, regex=True, inplace=True)\n",
    "nan_counts_per_column = lfs_data.isna().sum()\n",
    "print(nan_counts_per_column[nan_counts_per_column > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that these are -1, let's return to the data types, and find if our object columns from earlier are convertible to integers (or float):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_convertible_columns = []\n",
    "\n",
    "for col in lfs_data.columns:\n",
    "    if lfs_data[col].dtypes == 'object':  \n",
    "        try:\n",
    "            float_vals = lfs_data[col].dropna().astype(float)\n",
    "            if (float_vals % 1 == 0).all():\n",
    "                int_convertible_columns.append(col)\n",
    "        except ValueError:\n",
    "            pass \n",
    "\n",
    "print(\"Safely convertable to int:\")\n",
    "print(int_convertible_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "And convert to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = [\n",
    "    'PUFC06_MSTAT', 'PUFC08_CURSCH', 'PUFC09_GRADTECH', 'PUFC10_CONWR', 'PUFC11_WORK', \n",
    "    'PUFC12_JOB', 'PUFC14_PROCC', 'PUFC16_PKB', 'PUFC17_NATEM', 'PUFC18_PNWHRS', \n",
    "    'PUFC19_PHOURS', 'PUFC20_PWMORE', 'PUFC21_PLADDW', 'PUFC22_PFWRK', 'PUFC23_PCLASS', \n",
    "    'PUFC24_PBASIS', 'PUFC25_PBASIC', 'PUFC26_OJOB', 'PUFC27_NJOBS', 'PUFC28_THOURS', \n",
    "    'PUFC29_WWM48H', 'PUFC30_LOOKW', 'PUFC31_FLWRK', 'PUFC32_JOBSM', 'PUFC33_WEEKS', \n",
    "    'PUFC34_WYNOT', 'PUFC35_LTLOOKW', 'PUFC36_AVAIL', 'PUFC37_WILLING', 'PUFC38_PREVJOB', \n",
    "    'PUFC40_POCC', 'PUFC41_WQTR', 'PUFC43_QKB', 'PUFNEWEMPSTAT'\n",
    "]\n",
    "\n",
    "for col in columns_to_convert:\n",
    "    lfs_data[col] = lfs_data[col].astype(int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's also apply the unique() function to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_data.apply(lambda x: x.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Considering our dataset has 18,000 entries, features with particularly low numbers stand out as questions that have clear, defined choices. Reviewing the [questionnaire](https://psada.psa.gov.ph/catalog/67/download/537), we find that certain questions ask the participant to specify beyond prespecified choices.\n",
    "\n",
    "This column possibly contains \"010,\" which is obviously not an integer. We ensure this column is a string, and check for values not specified in the questionnaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_data['PUFC07_GRADE'] = lfs_data['PUFC07_GRADE']\n",
    "valid_codes = [\n",
    "    0, 10,  # No Grade, Preschool\n",
    "    210, 220, 230, 240, 250, 260, 280,  # Elementary\n",
    "    310, 320, 330, 340, 350,  # High School\n",
    "    410, 420,  # Post Secondary; If Graduate Specify\n",
    "    810, 820, 830, 840,  # College; If Graduate Specify\n",
    "    900,  # Post Baccalaureate\n",
    "    np.nan\n",
    "]\n",
    "invalid_rows = lfs_data[~(lfs_data['PUFC07_GRADE'].isin(valid_codes))]\n",
    "\n",
    "unique_invalid_values = invalid_rows['PUFC07_GRADE'].unique()\n",
    "print(unique_invalid_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values 5XX 6XX are not detailed in the questionnaire. As it instructs the participant to specify whether they graduated from post secondary or college, we'll create a new data point to encapsulate these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_data.loc[~lfs_data['PUFC07_GRADE'].isin(valid_codes), 'PUFC07_GRADE'] = 700\n",
    "print(lfs_data['PUFC07_GRADE'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_data['PUFC11_WORK'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_data_with_nan = lfs_data.copy()\n",
    "lfs_data_with_nan.replace(-1, np.nan, inplace=True)\n",
    "corr_matrix = lfs_data_with_nan.corr()\n",
    "\n",
    "strong_correlations = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i + 1, len(corr_matrix.columns)): \n",
    "        corr_value = corr_matrix.iloc[i, j]\n",
    "        if (0.5 < corr_value < 1) or (-1 < corr_value < -0.5):\n",
    "            strong_correlations.append((\n",
    "                corr_matrix.index[i], \n",
    "                corr_matrix.columns[j], \n",
    "                corr_value\n",
    "            ))\n",
    "\n",
    "strong_correlations.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "print(\"Strong correlations (|corr| > 0.5 and |corr| < 1):\")\n",
    "for var1, var2, corr in strong_correlations:\n",
    "    print(f\"{var1} â€” {var2}: {corr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use feature_cols as our predictor variables for PUFC11_WORK. Using Regression, we will evaluate how well we can predict whether or not someone has worked in the past week. We begin with preprocessing.\n",
    "\n",
    "### Logistic Regression (LR)\n",
    "Because this is a classification task, we can choose Logistic Regression as a baseline and a first model. In theory, PUFC11_WORK is a binary classification task, something that Logistic Regression should excel at. Since this uses a sigmoid function to map linear probabilities, between 0 and 1, making this suitable for determining whether someone \"worked\" (1) or \"did not work\" (0) within the past week. Finally, LR is particularly interpetable considering the task we're doing, allowing us to check the significance and coefficients associated with each predictor variable if need be.\n",
    "\n",
    "#### Preprocessing: LR\n",
    "We create a copy of our target variable PUFC11_WORK, and create five 80-20 train-test splits for the non-empty PUFC11 data we have. We'll also perform one-hot encoding, given our feature columns are all categorical. We also map the values of PUFC11, normally [1,2], to [0,1]. In addition, because we've determined that our feature_cols are intentionally unanswered as appropriate, we will be treating these values as empty rather than imputing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import prepare_data_kfold\n",
    "\n",
    "target_col = 'PUFC11_WORK'\n",
    "feature_cols = [\n",
    "    'PUFC05_AGE', 'PUFC06_MSTAT', 'PUFC04_SEX', \n",
    "    'PUFC07_GRADE', 'PUFC08_CURSCH', \n",
    "    'PUFC38_PREVJOB', 'PUFC31_FLWRK',\n",
    "    'PUFC30_LOOKW', 'PUFC34_WYNOT'\n",
    "]\n",
    "\n",
    "categorical_cols = feature_cols\n",
    "n_splits = 5\n",
    "missing_value =-1\n",
    "seed = 45\n",
    "\n",
    "folds_data = prepare_data_kfold(lfs_data, target_col = target_col,\n",
    "                         n_splits = n_splits,\n",
    "                         missing_value = missing_value,\n",
    "                         categorical_cols = categorical_cols,\n",
    "                         feature_cols = feature_cols,\n",
    "                         seed = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training: LR\n",
    "We are ready to start training. As we are setting a baseline, we'll start off with these hyperparameters. We'll use Stochastic Gradient Descent as our specified optimizer, though technically implementing mini-batch with a batch size of 128. We are looking for improvements in loss greater than 0.0001, else we stop early within three epochs. Lastly, we use a weight_decay of 0, indicating no regularization for now. All hyperparameters indicated below also indicate their default value if unspecified. For each convergence, we will track the metrics of that fold and aggregate it across all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainEval import *\n",
    "result_dict = train_model(\n",
    "    folds_data,\n",
    "    optimizer_string=\"sgd\", #Stochastic Gradient Descent\n",
    "    scheduler_step_size=5,\n",
    "    learning_rate=0.01,\n",
    "    scheduler_gamma=0.5,\n",
    "    convergence_threshold=1e-4, \n",
    "    num_epochs=50,\n",
    "    patience=3, # Stop at 3 epochs with no improvement\n",
    "    weight_decay=0,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nModel training complete!\")\n",
    "aggregate_cm = result_dict[\"aggregate_confusion_matrix\"]\n",
    "\n",
    "sns.heatmap(aggregate_cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=[\"Negative\", \"Positive\"], \n",
    "            yticklabels=[\"Negative\", \"Positive\"]) \n",
    "\n",
    "\n",
    "print(f\"  Average Final Train Loss:     {result_dict['aggregated_final_metrics']['avg_final_train_loss']:.6f}\")\n",
    "print(f\"  Average Final Test Loss:      {result_dict['aggregated_final_metrics']['avg_final_test_loss']:.6f}\")\n",
    "print(f\"  Average Final Train Accuracy: {result_dict['aggregated_final_metrics']['avg_final_train_accuracy']:.6f}\")\n",
    "print(f\"  Average Final Test Accuracy:  {result_dict['aggregated_final_metrics']['avg_final_test_accuracy']:.6f}\")\n",
    "\n",
    "plt.title(\"Aggregate Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error: LR\n",
    "This model performed well for an initial run. All folds ran for 25 epochs. Both accuracies for test and train data are high, though there is usually minimal improvement from the third epoch onwards. The train and test loss both see steady improvement, but we see the test loss  to be slightly higher and plateau earlier at around the sixteenth epoch. \n",
    "\n",
    "We see that the train loss is slightly lower than the test loss in folds 1, 2, and 5, while the reverse is true for folds 3 and 4. This would indicate a model that overfits for the former folds, and underfits for the latter. Averaging these out, its clear that with an aggregated train and test loss of 0.684 and 0.687 respectively, the overall performance of the model doesn't significantly overfit nor underfit. This outcome is surprising, considering that we've used nine predictor variables while also forgoing the use of regularization, as these would have been expected to introduce noise and contribute to overfitting through curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning: LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our base parameters having been set, we can begin hyperparameter tuning. We'll implement *Random Search* as to not have to exhaustively branch through each possible combination of parameters. Below are the hyperparameter's we'll use and choose from. For example, if our algorithm randomly, for instance, chooses the second element of each, then it will test Logistic Regression at a learning rate of 0.1, batch size of 64, and the with the ADAM optimizer, etc. \n",
    "\n",
    "Values evenly spaced between 0.000001 and 0.001 will be explored for the learning rates. This will go on for fifty iterations, afterwhich a best model will be selected based on test accuracy. We'll perform random search rather than grid search, as a compromise to searching through all 259,200 combinations of hyperparameters in our grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'learning_rate': np.logspace(-4, -1, 20),\n",
    "    'batch_size': [32, 64, 128, 256],\n",
    "    'optimizer': ['sgd', 'adam', 'rmsprop'],\n",
    "    'weight_decay': np.logspace(-5, -2, 10),\n",
    "    'num_epochs': [30, 50, 75, 100],\n",
    "    'scheduler_step_size': [5, 10, 15],\n",
    "    'scheduler_gamma': [0.5, 0.7, 0.9],\n",
    "    'patience': [3, 5, 7]\n",
    "}\n",
    "\n",
    "hyperparameter_results = hyperparameter_random_search(folds_data=folds_data, param_distributions=param_distributions, n_iter_search=50, seed=seed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPT Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##best_params = hyperparameter_results['best_params']\n",
    "##results_summary = hyperparameter_results['summary_df']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree (DT)\n",
    "\n",
    "Another machine learning model we can use is the decision tree. Decision trees don't necessarily assume a linear relationship between our predictor variables and PUFC11, which could be benificial for finding complex relationships. Rather, they recursively partition our data with rules. Further, while it can get quite complex as it scales, individual nodes are also uncomplicated in how they are interpreted, which is also helpful for understanding the factors that determine whether an individual has worked within the past week.\n",
    "\n",
    "#### Training: DT\n",
    "We can recycle the one-hot encoded data we used in preparation for Logistic Regression and immediately use it for training our decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, log_loss, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "accuracies_train = []\n",
    "losses_train = []\n",
    "accuracies_test = []\n",
    "losses_test = []\n",
    "confusion_matrices = [] \n",
    "\n",
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = (\n",
    "        folds_data[i]['X_train'],\n",
    "        folds_data[i]['X_test'],\n",
    "        folds_data[i]['y_train'],\n",
    "        folds_data[i]['y_test'],\n",
    "    )\n",
    "\n",
    "    model = DecisionTreeClassifier(random_state=45, min_impurity_decrease=0.001)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Train predictions and loss\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_train_pred_proba = model.predict_proba(X_train)\n",
    "    accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "    loss_train = log_loss(y_train, y_train_pred_proba)\n",
    "\n",
    "    # Test predictions and loss\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred_proba = model.predict_proba(X_test)\n",
    "    accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "    loss_test = log_loss(y_test, y_test_pred_proba)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    confusion_matrices.append(cm)\n",
    "\n",
    "    accuracies_train.append(accuracy_train)\n",
    "    losses_train.append(loss_train)\n",
    "    accuracies_test.append(accuracy_test)\n",
    "    losses_test.append(loss_test)\n",
    "\n",
    "    print(f\"Fold {i+1} - Train Accuracy: {accuracy_train:.4f}, Train Loss: {loss_train:.4f}, Test Accuracy: {accuracy_test:.4f}, Test Loss: {loss_test:.4f}\")\n",
    "    print(f\"Fold {i+1} - Confusion Matrix:\\n{cm}\\n\")\n",
    "\n",
    "avg_accuracy_train = np.mean(accuracies_train)\n",
    "avg_loss_train = np.mean(losses_train)\n",
    "avg_accuracy_test = np.mean(accuracies_test)\n",
    "avg_loss_test = np.mean(losses_test)\n",
    "\n",
    "print(f\"\\nAggregated Metrics:\")\n",
    "print(f\"Average Train Accuracy: {avg_accuracy_train:.4f}\")\n",
    "print(f\"Average Train Loss: {avg_loss_train:.4f}\")\n",
    "print(f\"Average Test Accuracy: {avg_accuracy_test:.4f}\")\n",
    "print(f\"Average Test Loss: {avg_loss_test:.4f}\")\n",
    "\n",
    "print(\"\\nOverall Confusion Matrix:\")\n",
    "print(sum(confusion_matrices))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error: DT\n",
    "Our model is also performing well, and with each confusion metric no less than 0.98. As this is a decision tree, a model usually prone to overfitting, we do see minimal overfitting, seeing as our train loss is higher than our test loss by 0.0023, though this is arguably a negligible gap.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning: DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to our methodology with Logistic Regression, we will implement random search for our tuning as a compromise to exhaustively searching through each hyperparameter combination. This time, we will explicitly define certain thresholds and measures. \n",
    "\n",
    "Our previous two hyperparameters, max_depth and minimum_impurity_decrease, will now range fom 30-60 and 0.001 to 0.1 respectively, adding variability to our parameters despite minimal signs of overfitting. We will allow the min_samples_leaf to enforce generalization or allow leaf nodes with one sample. \n",
    "\n",
    "Of note, we will add cost-complexity pruning as a form of regularizaton for our decision trees. We will also be exploring both gini and entropy as criteria, as the former minimizes chances of misclassification while the latter measures information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error: DT\n",
    "Our model is also performing well, and with each confusion metric no less than 0.98. As this is a decision tree, a model usually prone to overfitting, we do see minimal overfitting, seeing as our train loss is higher than our test loss by 0.0023, though this is arguably a negligible gap.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "\n",
    "p_grid = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [30, 45, 50, 60],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"min_impurity_decrease\": [0.001, 0.005, 0.01],\n",
    "    \"ccp_alpha\": np.logspace(-4, 0, 10)\n",
    "}\n",
    "\n",
    "best_params_list = []\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "losses = []\n",
    "\n",
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = (\n",
    "        folds_data[i]['X_train'],\n",
    "        folds_data[i]['X_test'],\n",
    "        folds_data[i]['y_train'],\n",
    "        folds_data[i]['y_test'],\n",
    "    )\n",
    "\n",
    "    model = DecisionTreeClassifier(random_state=45)\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        model, p_grid, n_iter=20, cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=1, random_state=45\n",
    "    )\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params_list.append(random_search.best_params_)\n",
    "\n",
    "    accuracies.append(accuracy_score(y_test, random_search.best_estimator_.predict(X_test)))\n",
    "    report = classification_report(y_test, random_search.best_estimator_.predict(X_test), output_dict=True, zero_division=0)\n",
    "    losses.append(log_loss(y_test, random_search.best_estimator_.predict_proba(X_test)))\n",
    "    precisions.append(report['macro avg']['precision'])\n",
    "    recalls.append(report['macro avg']['recall'])\n",
    "    f1_scores.append(report['macro avg']['f1-score'])\n",
    "\n",
    "    print(f\"Fold {i+1} - Best Parameters: {random_search.best_params_}\")\n",
    "    print(f\"Fold {i+1} - Accuracy: {accuracies[-1]:.4f}, Precision: {precisions[-1]:.4f}, Recall: {recalls[-1]:.4f}, F1-score: {f1_scores[-1]:.4f}, Loss: {losses[-1]:.4f}\")\n",
    "\n",
    "\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_precision = np.mean(precisions)\n",
    "avg_recall = np.mean(recalls)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "avg_loss = np.mean(losses)\n",
    "\n",
    "print(f\"\\nAggregated Metrics:\")\n",
    "print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average F1-score: {avg_f1:.4f}\")\n",
    "print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "print(\"\\nBest Parameters for each fold:\")\n",
    "for i, params in enumerate(best_params_list):\n",
    "    print(f\"Fold {i+1}: {params}\")\n",
    "\n",
    "\n",
    "best_fold_index = np.argmax(accuracies)\n",
    "best_fold_params = best_params_list[best_fold_index]\n",
    "print(\"\\nBest Performing Fold Parameters:\", best_fold_params)\n",
    "\n",
    "\n",
    "best_model = DecisionTreeClassifier(random_state=45, **best_fold_params)\n",
    "\n",
    "\n",
    "all_X_train = np.concatenate([folds_data[i]['X_train'] for i in range(5)])\n",
    "all_y_train = np.concatenate([folds_data[i]['y_train'] for i in range(5)])\n",
    "\n",
    "best_model.fit(all_X_train, all_y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, log_loss, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "train_preds = best_model.predict(X_train)\n",
    "train_acc = accuracy_score(y_train, train_preds)\n",
    "\n",
    "test_preds = best_model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, test_preds)\n",
    "\n",
    "train_probs = best_model.predict_proba(X_train)\n",
    "test_probs = best_model.predict_proba(X_test)\n",
    "train_loss = log_loss(y_train, train_probs)\n",
    "test_loss = log_loss(y_test, test_probs)\n",
    "\n",
    "cm = confusion_matrix(y_test, test_preds)  \n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, \n",
    "            xticklabels=[\"Predicted Negative\", \"Predicted Positive\"], \n",
    "            yticklabels=[\"Actual Negative\", \"Actual Positive\"])\n",
    "\n",
    "\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(cm)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "print(f\"Training Log Loss: {train_loss:.4f}\")\n",
    "print(f\"Test Log Loss: {test_loss:.4f}\")\n",
    "\n",
    "\n",
    "train_precision = precision_score(y_train, train_preds, zero_division=0)\n",
    "test_precision = precision_score(y_test, test_preds, zero_division=0)\n",
    "\n",
    "train_recall = recall_score(y_train, train_preds, zero_division=0)\n",
    "test_recall = recall_score(y_test, test_preds, zero_division=0)\n",
    "\n",
    "train_f1 = f1_score(y_train, train_preds, zero_division=0)\n",
    "test_f1 = f1_score(y_test, test_preds, zero_division=0)\n",
    "\n",
    "print(f\"Training Precision: {train_precision:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "\n",
    "print(f\"Training Recall: {train_recall:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "print(f\"Training F1-score: {train_f1:.4f}\")\n",
    "print(f\"Test F1-score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPT Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
