{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Introduction ##\n",
    "\n",
    "In this notebook, the dataset to be processed is the Labor Force Survey conducted April 2016 and retrieved through Philippine Statistics Authority database. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6.0, 6.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# autoreload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importing LFS PUF April 2016.CSV</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "try:\n",
    "    lfs_data = pd.read_csv(\"LFS PUF April 2016.CSV\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: CSV file not found. Please make sure the file exists in the correct directory or provide the correct path.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Information, Pre-Processing, and Cleaning</h1>\n",
    "\n",
    "Let's get an overview of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Of interest to us, there are:\n",
    "<ul><li>1 contains float values, </li>\n",
    "<li>14 contain integer values, and </li>\n",
    "<li><b>35 are object values</b>.</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicates here, and therefore no cleaning need follow in this regard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset seems to contain null values in the form of whitespaces. Let's count those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_null = lfs_data.apply(lambda col: col.str.isspace().sum() if col.dtype == 'object' else 0)\n",
    "\n",
    "print(\"Number Empty Cells:\")\n",
    "print(has_null[has_null > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "And standardize, replacing these whitespace values with NaN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_data.replace(r\"^\\s+$\", np.nan, regex=True, inplace=True)\n",
    "nan_counts_per_column = lfs_data.isna().sum()\n",
    "print(nan_counts_per_column[nan_counts_per_column > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's also apply the unique() function to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_data.apply(lambda x: x.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Considering our dataset has 18,000 entries, features with particularly low numbers stand out as questions that have clear, defined choices. Reviewing the [questionnaire](https://psada.psa.gov.ph/catalog/67/download/537), we find that certain questions ask the participant to specify beyond prespecified choices.\n",
    "\n",
    "This column possibly contains \"010,\" which is obviously not an integer. We ensure this column is a string, and check for values not specified in the questionnaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_data['PUFC07_GRADE'] = lfs_data['PUFC07_GRADE'].astype(str)\n",
    "valid_codes = [\n",
    "    \"000\", \"010\",                                      # No Grade, Preschool\n",
    "    \"210\", \"220\", \"230\", \"240\", \"250\", \"260\", \"280\",  # Elementary\n",
    "    \"310\", \"320\", \"330\", \"340\", \"350\",                # High School\n",
    "    \"410\", \"420\",                                     # Post Secondary; If Graduate Specify\n",
    "    \"810\", \"820\", \"830\", \"840\",                       # College; If Graduate Specify\n",
    "    \"900\",                                            # Post Baccalaureate\n",
    "    \"nan\"\n",
    "]\n",
    "invalid_rows = lfs_data[~(lfs_data['PUFC07_GRADE'].isin(valid_codes))]\n",
    "\n",
    "unique_invalid_values = invalid_rows['PUFC07_GRADE'].unique()\n",
    "print(unique_invalid_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values 5XX 6XX are not detailed in the questionnaire. As it instructs the participant to specify whether they graduated from post secondary or college, we'll create a new data point to encapsulate these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_data.loc[~lfs_data['PUFC07_GRADE'].isin(valid_codes), 'PUFC07_GRADE'] = '700'\n",
    "print(lfs_data['PUFC07_GRADE'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all columns to string first to avoid issues with LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_data=lfs_data.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use LabelEncoder to convert categorical variables to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "for col in ['PUFC09_GRADTECH', 'PUFC07_GRADE', 'PUFC08_CURSCH']:\n",
    "    lfs_data[col] = encoder.fit_transform(lfs_data[col])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into predictors (x) and target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lfs_data[['PUFC09_GRADTECH', 'PUFC07_GRADE', 'PUFC05_AGE']]\n",
    "y = lfs_data['PUFC08_CURSCH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits the dataset into 80% training and 20% testing sets.\n",
    "random state is the seed for the random integer generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)\n",
    "model = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=69)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train ðŸš‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=420)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
